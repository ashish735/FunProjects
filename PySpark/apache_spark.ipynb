{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597822830881",
   "display_name": "Python 3.7.7 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Apache Spark?\n",
    "https://www.guru99.com/pyspark-tutorial.html\n",
    "\n",
    "Spark is a big data solution that has been proven to be easier and faster than Hadoop MapReduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier tools like MapReduce were favorite but were slow. To overcome this issue, Spark offers a solution that is both fast and general-purpose. The main difference between Spark and MapReduce is that Spark runs computations in memory during the later on the hard disk. It allows high-speed access and data processing, reducing times from hours to minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Pyspark?\n",
    "\n",
    "Spark is the name of the engine to realize cluster computing while PySpark is the Python's library to use Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Does Spark work?\n",
    "\n",
    "Spark is based on computational engine, meaning it takes care of the scheduling, distributing and monitoring application. Each task is done across various worker machines called computing cluster. A computing cluster refers to the division of tasks. One machine performs one task, while the others contribute to the final output through a different task. In the end, all the tasks are aggregated to produce an output. The Spark admin gives a 360 overview of various Spark Jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is designed to work with\n",
    "\n",
    "    Python\n",
    "    Java\n",
    "    Scala\n",
    "    SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant feature of Spark is the vast amount of built-in library, including MLlib for machine learning. Spark is also designed to work with Hadoop clusters and can read the broad type of files, including Hive data, CSV, JSON, Casandra data among other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use Spark?\n",
    "\n",
    "As a future data practitioner, you should be familiar with python's famous libraries: Pandas and scikit-learn. These two libraries are fantastic to explore dataset up to mid-size. Regular machine learning projects are built around the following methodology:\n",
    "\n",
    "    Load the data to the disk\n",
    "    Import the data into the machine's memory\n",
    "    Process/analyze the data\n",
    "    Build the machine learning model\n",
    "    Store the prediction back to disk\n",
    "\n",
    "The problem arises if the data scientist wants to process data that's too big for one computer. During earlier days of data science, the practitioners would sample the as training on huge data sets was not always needed. The data scientist would find a good statistical sample, perform an additional robustness check and comes up with an excellent model.\n",
    "\n",
    "However, there are some problems with this:\n",
    "\n",
    "    Is the dataset reflecting the real world?\n",
    "    Does the data include a specific example?\n",
    "    Is the model fit for sampling?\n",
    "\n",
    "Take users recommendation for instance. Recommenders rely on comparing users with other users in evaluating their preferences. If the data practitioner takes only a subset of the data, there won't be a cohort of users who are very similar to one another. Recommenders need to run on the full dataset or not at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the solution?\n",
    "\n",
    "The solution has been evident for a long time, split the problem up onto multiple computers. Parallel computing comes with multiple problems as well. Developers often have trouble writing parallel code and end up having to solve a bunch of the complex issues around multi-processing itself.\n",
    "\n",
    "Pyspark gives the data scientist an API that can be used to solve the parallel data proceedin problems. Pyspark handles the complexities of multiprocessing, such as distributing the data, distributing code and collecting output from the workers on a cluster of machines.\n",
    "\n",
    "Spark can run standalone but most often runs on top of a cluster computing framework such as Hadoop. In test and development, however, a data scientist can efficiently run Spark on their development boxes or laptops without a cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main advantages of Spark is to build an architecture that encompasses data streaming management, seamlessly data queries, machine learning prediction and real-time access to various analysis.\n",
    "\n",
    "• Spark works closely with SQL language, i.e., structured data. It allows querying the data in real time.\n",
    "\n",
    "• Data scientist main's job is to analyze and build predictive models. In short, a data scientist needs to know how to query data using SQL, produce a statistical report and make use of machine learning to produce predictions. Data scientist spends a significant amount of their time on cleaning, transforming and analyzing the data. Once the dataset or data workflow is ready, the data scientist uses various techniques to discover insights and hidden patterns. The data manipulation should be robust and the same easy to use. Spark is the right tool thanks to its speed and rich APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of this writing, PySpark is not compatible with Java9 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning with Spark\n",
    "\n",
    "Now that you have a brief idea of Spark and SQLContext, you are ready to build your first Machine learning program.\n",
    "\n",
    "You will proceed as follow:\n",
    "\n",
    "    Step 1) Basic operation with PySpark\n",
    "    Step 2) Data preprocessing\n",
    "    Step 3) Build a data processing pipeline\n",
    "    Step 4) Build the classifier\n",
    "    Step 5) Train and evaluate the model\n",
    "    Step 6) Tune the hyperparameter\n",
    "\n",
    "In this tutorial, we will use the adult dataset. The purpose of this tutorial is to learn how to use Pyspark. For more information about the dataset, refer to this tutorial.\n",
    "\n",
    "Note that, the dataset is not significant and you may think that the computation takes a long time. Spark is designed to process a considerable amount of data. Spark's performances increase relative to other machine learning libraries when the dataset processed grows larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"ML\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, you can read the cvs file with sqlContext.read.csv. You use inferSchema set to True to tell Spark to guess automatically the type of data. By default, it is turn to False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- x: integer (nullable = true)\n |-- age: integer (nullable = true)\n |-- workclass: string (nullable = true)\n |-- fnlwgt: integer (nullable = true)\n |-- education: string (nullable = true)\n |-- educational-num: integer (nullable = true)\n |-- marital-status: string (nullable = true)\n |-- occupation: string (nullable = true)\n |-- relationship: string (nullable = true)\n |-- race: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- capital-gain: integer (nullable = true)\n |-- capital-loss: integer (nullable = true)\n |-- hours-per-week: integer (nullable = true)\n |-- native-country: string (nullable = true)\n |-- income: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n|x  |age|workclass|fnlwgt|education   |educational-num|marital-status    |occupation       |relationship|race |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n|1  |25 |Private  |226802|11th        |7              |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0           |0           |40            |United-States |<=50K |\n|2  |38 |Private  |89814 |HS-grad     |9              |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0           |0           |50            |United-States |<=50K |\n|3  |28 |Local-gov|336951|Assoc-acdm  |12             |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0           |0           |40            |United-States |>50K  |\n|4  |44 |Private  |160323|Some-college|10             |Married-civ-spouse|Machine-op-inspct|Husband     |Black|Male  |7688        |0           |40            |United-States |>50K  |\n|5  |18 |?        |103497|Some-college|10             |Never-married     |?                |Own-child   |White|Female|0           |0           |30            |United-States |<=50K |\n+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't set inderShema to True, here is what is happening to the type. There are all in string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+\n|age|fnlwgt|\n+---+------+\n| 25|226802|\n| 38| 89814|\n| 28|336951|\n| 44|160323|\n| 18|103497|\n+---+------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "df.select('age','fnlwgt').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------+-----+\n|   education|count|\n+------------+-----+\n|   Preschool|   83|\n|     1st-4th|  247|\n|     5th-6th|  509|\n|   Doctorate|  594|\n|        12th|  657|\n|         9th|  756|\n| Prof-school|  834|\n|     7th-8th|  955|\n|        10th| 1389|\n|  Assoc-acdm| 1601|\n|        11th| 1812|\n|   Assoc-voc| 2061|\n|     Masters| 2657|\n|   Bachelors| 8025|\n|Some-college|10878|\n|     HS-grad|15784|\n+------------+-----+\n\n"
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n|summary|                 x|               age|  workclass|            fnlwgt|   education|   educational-num|marital-status|      occupation|relationship|              race|gender|      capital-gain|     capital-loss|    hours-per-week|native-country|income|\n+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n|  count|             48842|             48842|      48842|             48842|       48842|             48842|         48842|           48842|       48842|             48842| 48842|             48842|            48842|             48842|         48842| 48842|\n|   mean|           24421.5| 38.64358543876172|       null|189664.13459727284|        null|10.078088530363212|          null|            null|        null|              null|  null|1079.0676262233324|87.50231358257237|40.422382375824085|          null|  null|\n| stddev|14099.615260708357|13.710509934443525|       null|105604.02542315757|        null| 2.570972755592259|          null|            null|        null|              null|  null| 7452.019057655418| 403.004552124359|12.391444024252312|          null|  null|\n|    min|                 1|                17|          ?|             12285|        10th|                 1|      Divorced|               ?|     Husband|Amer-Indian-Eskimo|Female|                 0|                0|                 1|             ?| <=50K|\n|    max|             48842|                90|Without-pay|           1490400|Some-college|                16|       Widowed|Transport-moving|        Wife|             White|  Male|             99999|             4356|                99|    Yugoslavia|  >50K|\n+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n\n"
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+------------------+\n|summary|               age|\n+-------+------------------+\n|  count|             48842|\n|   mean| 38.64358543876172|\n| stddev|13.710509934443525|\n|    min|                17|\n|    max|                90|\n+-------+------------------+\n\n"
    }
   ],
   "source": [
    "df.describe('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------------+-------+-------------+--------------+---------+---------+----+\n|age_relationship|Husband|Not-in-family|Other-relative|Own-child|Unmarried|Wife|\n+----------------+-------+-------------+--------------+---------+---------+----+\n|              17|      1|           22|            30|      536|        6|   0|\n|              18|      5|           74|            47|      721|       10|   5|\n|              19|      9|          177|            83|      745|       31|   8|\n|              20|     30|          231|            81|      722|       43|   6|\n|              21|     50|          241|            81|      666|       44|  14|\n|              22|     85|          343|            74|      565|       83|  28|\n|              23|    141|          463|            64|      544|       93|  24|\n|              24|    182|          436|            55|      383|       96|  54|\n|              25|    213|          417|            62|      348|      107|  48|\n|              26|    260|          446|            65|      237|       95|  50|\n|              27|    322|          435|            61|      250|      104|  60|\n|              28|    383|          440|            52|      218|      117|  70|\n|              29|    394|          422|            41|      154|      137|  75|\n|              30|    475|          394|            46|      148|      146|  69|\n|              31|    529|          391|            40|      135|      145|  85|\n|              32|    553|          335|            42|      115|      140|  68|\n|              33|    590|          374|            25|      111|      157|  78|\n|              34|    591|          329|            22|      110|      172|  79|\n|              35|    643|          342|            25|       93|      164|  70|\n|              36|    628|          321|            24|       89|      197|  89|\n+----------------+-------+-------------+--------------+---------+---------+----+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df.crosstab('age', 'relationship').sort(\"age_relationship\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2) Data preprocessing\n",
    "\n",
    "Data processing is a critical step in machine learning. After you remove garbage data, you get some important insights. For instance, you know that age is not a linear function with the income. When people are young, their income is usually lower than mid-age. After retirement, a household uses their saving, meaning a decrease in income. To capture this pattern, you can add a square to the age feature\n",
    "\n",
    "Add age square\n",
    "\n",
    "To add a new feature, you need to:\n",
    "\n",
    "    Select the column\n",
    "    Apply the transformation and add it to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- x: integer (nullable = true)\n |-- age: integer (nullable = true)\n |-- workclass: string (nullable = true)\n |-- fnlwgt: integer (nullable = true)\n |-- education: string (nullable = true)\n |-- educational-num: integer (nullable = true)\n |-- marital-status: string (nullable = true)\n |-- occupation: string (nullable = true)\n |-- relationship: string (nullable = true)\n |-- race: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- capital-gain: integer (nullable = true)\n |-- capital-loss: integer (nullable = true)\n |-- hours-per-week: integer (nullable = true)\n |-- native-country: string (nullable = true)\n |-- income: string (nullable = true)\n |-- age_square: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1 Select the column\n",
    "age_square = df.select(col(\"age\")**2)\n",
    "\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age_square\", col(\"age\")**2)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Build a data processing pipeline\n",
    "\n",
    "Similar to scikit-learn, Pyspark has a pipeline API. A pipeline is very convenient to maintain the structure of the data. You push the data into the pipeline. Inside the pipeline, various operations are done, the output is used to feed the algorithm.\n",
    "\n",
    "For instance, one universal transformation in machine learning consists of converting a string to one hot encoder, i.e., one column by a group. One hot encoder is usually a matrix full of zeroes.\n",
    "\n",
    "The steps to transform the data are very similar to scikit-learn. You need to:\n",
    "\n",
    "    Index the string to numeric\n",
    "    Create the one hot encoder\n",
    "    Transform the data\n",
    "\n",
    "Two APIs do the job: StringIndexer, OneHotEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you select the string column to index. The inputCol is the name of the column in the dataset. outputCol is the new name given to the transformed column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the data and transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stringIndexer.fit(df)\t\t\n",
    "indexed = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the news columns based on the group. For instance, if there are 10 groups in the feature, the new matrix will have 10 columns, one for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder(dropLast=False, inputCol=\"workclassencoded\", outputCol=\"workclassvec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---------+------+---------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+-----------------+-------------+\n|  x|age|workclass|fnlwgt|education|educational-num|    marital-status|       occupation|relationship| race|gender|capital-gain|capital-loss|hours-per-week|native-country|income|age_square|workclass_encoded|workclass_vec|\n+---+---+---------+------+---------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+-----------------+-------------+\n|  1| 25|  Private|226802|     11th|              7|     Never-married|Machine-op-inspct|   Own-child|Black|  Male|           0|           0|            40| United-States| <=50K|     625.0|              0.0|(9,[0],[1.0])|\n|  2| 38|  Private| 89814|  HS-grad|              9|Married-civ-spouse|  Farming-fishing|     Husband|White|  Male|           0|           0|            50| United-States| <=50K|    1444.0|              0.0|(9,[0],[1.0])|\n+---+---+---------+------+---------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+-----------------+-------------+\nonly showing top 2 rows\n\n"
    }
   ],
   "source": [
    "### Example encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\")\n",
    "encoder = encoder.fit(indexed)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the pipeline\n",
    "\n",
    "You will build a pipeline to convert all the precise features and add them to the final dataset. The pipeline will have four operations, but feel free to add as many operations as you want.\n",
    "\n",
    "    Encode the categorical data\n",
    "    Index the label feature\n",
    "    Add continuous variable\n",
    "    Assemble the steps.\n",
    "\n",
    "Each step is stored in a list named stages. This list will tell the VectorAssembler what operation to perform inside the pipeline.\n",
    "\n",
    "1. Encode the categorical data\n",
    "\n",
    "This step is exaclty the same as the above example, except that you loop over all the categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Index the label feature\n",
    "\n",
    "Spark, like many other libraries, does not accept string values for the label. You convert the label feature with StringIndexer and add it to the list stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"income\", outputCol=\"newincome\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add continuous variable\n",
    "\n",
    "The inputCols of the VectorAssembler is a list of columns. You can create a new list containing all the new columns. The code below popluate the list with encoded categorical features and the continuous features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital-gain', 'educational-num', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assemble the steps.\n",
    "\n",
    "Finally, you pass all the steps in the VectorAssembler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the steps are ready, you push the data to the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "model = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the new dataset, you can see that it contains all the features, transformed and not transformed. You are only interested by the newlabel and features. The features includes all the transformed features and the continuous variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(x=1, age=25, workclass='Private', fnlwgt=226802, education='11th', educational-num=7, marital-status='Never-married', occupation='Machine-op-inspct', relationship='Own-child', race='Black', gender='Male', capital-gain=0, capital-loss=0, hours-per-week=40, native-country='United-States', income='<=50K', age_square=625.0, workclassIndex=0.0, workclassclassVec=SparseVector(8, {0: 1.0}), educationIndex=5.0, educationclassVec=SparseVector(15, {5: 1.0}), marital-statusIndex=1.0, marital-statusclassVec=SparseVector(6, {1: 1.0}), occupationIndex=6.0, occupationclassVec=SparseVector(14, {6: 1.0}), relationshipIndex=2.0, relationshipclassVec=SparseVector(5, {2: 1.0}), raceIndex=1.0, raceclassVec=SparseVector(4, {1: 1.0}), genderIndex=0.0, genderclassVec=SparseVector(1, {0: 1.0}), native-countryIndex=0.0, native-countryclassVec=SparseVector(41, {0: 1.0}), newincome=0.0, features=SparseVector(100, {0: 1.0, 13: 1.0, 24: 1.0, 35: 1.0, 45: 1.0, 49: 1.0, 52: 1.0, 53: 1.0, 94: 25.0, 95: 226802.0, 97: 7.0, 99: 40.0}))]"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4) Build the classifier: logistic\n",
    "\n",
    "To make the computation faster, you convert model to a DataFrame. You need to select newlabel and features from model using map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newincome\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to create the train data as a DataFrame. You use the sqlContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sqlContext.createDataFrame(input_data, [\"income\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+--------------------+\n|income|            features|\n+------+--------------------+\n|   0.0|[1.0,0.0,0.0,0.0,...|\n|   0.0|[1.0,0.0,0.0,0.0,...|\n+------+--------------------+\nonly showing top 2 rows\n\n"
    }
   ],
   "source": [
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/test set\n",
    "\n",
    "You split the dataset 80/20 with randomSplit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many people with income below/above 50k in both training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+-------------+\n|income|count(income)|\n+------+-------------+\n|   0.0|        29731|\n|   1.0|         9295|\n+------+-------------+\n\n"
    }
   ],
   "source": [
    "train_data.groupby('income').agg({'income': 'count'}).show()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+-------------+\n|income|count(income)|\n+------+-------------+\n|   0.0|         7424|\n|   1.0|         2392|\n+------+-------------+\n\n"
    }
   ],
   "source": [
    "test_data.groupby('income').agg({'income': 'count'}).show()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the logistic regressor\n",
    "\n",
    "Last but not least, you can build the classifier. Pyspark has an API called LogisticRegression to perform logistic regression.\n",
    "\n",
    "You initialize lr by indicating the label column and feature columns. You set a maximum of 10 iterations and add a regularization parameter with a value of 0.3. Note that in the next section, you will use cross-validation with a parameter grid to tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"income\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficients: [-0.062290459463056475,-0.1596092472910736,-0.06575267374093999,-0.15692179895946792,-0.1272492548940923,0.1645720986637153,0.19207847304400286,-0.2634703312602943,-0.19262480707620794,-0.06530587930152512,0.22083028208154978,0.3703503304642222,-0.009029093914692343,-0.2952283126280717,0.016111973903526002,-0.3352553665206933,-0.436755581550605,0.5366020022693624,-0.40896416018289755,-0.18468796676651497,0.603982838076872,-0.34140682703931285,-0.37948075355152383,0.3279778384592752,-0.3479105587484022,-0.20337271152379682,-0.21453545879299532,-0.13431514394370955,-0.15157587937021094,0.19010797769160911,-0.06868428196292928,0.2995150011312705,-0.12428164472354983,0.042102538721418954,-0.28154545879138165,-0.19453938122902198,-0.1582091687368907,-0.1340897761328162,-0.28536922494593336,-0.3580558840220062,0.12797378841052606,0.10738329738201971,-0.3008495785218508,0.2724039232382477,-0.19373670821554728,-0.2895726784347856,-0.24501638986086982,0.4349447725931794,-0.05509281140383402,-0.18012766308161565,-0.0765101783497171,-0.2617258090779879,0.17372838131661333,-0.11190298562097398,-0.3908668671224795,-0.18923890302986227,-0.016464841473206825,-0.14741606656987866,-0.25554767635784453,0.024764417862391863,-0.258558392804337,-0.05465453687695151,-0.1390967052528045,0.06912543700093315,-0.22951162376419515,-0.3858569677086675,-0.13600728501097767,-0.022822864217778488,-0.3691429492721224,-0.029972809227312967,-0.32940416622416097,-0.30391071398501895,-0.3378471569370846,-0.5348176540575098,-0.15422117649604858,0.006533510054542825,-0.09508149916815818,-0.05284989994439106,-0.12833588687388667,-0.3347450833071593,-0.4454285685000526,-0.2851983164928258,0.13624606321646682,0.12213448118796161,-0.36923439763076193,-0.34804494538471775,0.2236790573505897,-0.35578136070952887,-0.37042236743740126,-0.5504842492483373,0.5538155623725817,-0.40062298612275254,-0.24070625167645554,0.10099955314398502,0.006767313428448534,1.5836393047584385e-07,2.1659616705709016e-05,0.02862419038499012,0.00022787052356344946,0.008935653236560245]\nIntercept: -2.070148797796869\n"
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5) Train and evaluate the model\n",
    "\n",
    "To generate prediction for your test set, you can use linearModel with transform() on test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- income: double (nullable = true)\n |-- features: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are interested by the label, prediction and the probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+----------+--------------------+\n|income|prediction|         probability|\n+------+----------+--------------------+\n|   0.0|       0.0|[0.93098566170068...|\n|   0.0|       0.0|[0.92369259506918...|\n|   0.0|       1.0|[0.35954621838240...|\n|   0.0|       0.0|[0.91064720394777...|\n|   0.0|       0.0|[0.88933661204705...|\n|   0.0|       0.0|[0.64834756972696...|\n|   0.0|       0.0|[0.66262040852769...|\n|   0.0|       1.0|[0.44841664127453...|\n|   0.0|       0.0|[0.84050868469418...|\n|   0.0|       0.0|[0.84999336882262...|\n|   0.0|       0.0|[0.72698078269065...|\n|   0.0|       0.0|[0.84158214926617...|\n|   0.0|       0.0|[0.75952643445408...|\n|   0.0|       0.0|[0.83301890740706...|\n|   0.0|       0.0|[0.79353029070515...|\n|   0.0|       0.0|[0.81356864076497...|\n|   0.0|       0.0|[0.85152568791280...|\n|   0.0|       0.0|[0.87866998095111...|\n|   0.0|       0.0|[0.83846221320186...|\n|   0.0|       0.0|[0.63312826371495...|\n+------+----------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "selected = predictions.select(\"income\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model\n",
    "\n",
    "You need to look at the accuracy metric to see how well (or bad) the model performs. Currently, there is no API to compute the accuracy measure in Spark. The default value is the ROC, receiver operating characteristic curve. It is a different metrics that take into account the false positive rate.\n",
    "\n",
    "Before you look at the ROC, let's construct the accuracy measure. You are more familiar with this metric. The accuracy measure is the sum of the correct prediction over the total number of observations.\n",
    "\n",
    "You create a DataFrame with the label and the `prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+-------------+\n|income|count(income)|\n+------+-------------+\n|   0.0|         7424|\n|   1.0|         2392|\n+------+-------------+\n\n"
    }
   ],
   "source": [
    "cm = predictions.select(\"income\", \"prediction\")\n",
    "cm.groupby('income').agg({'income': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+-----------------+\n|prediction|count(prediction)|\n+----------+-----------------+\n|       0.0|             8842|\n|       1.0|              974|\n+----------+-----------------+\n\n"
    }
   ],
   "source": [
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in the test set, there is 1578 household with an income above 50k and 5021 below. The classifier, however, predicted 617 households with income above 50k.\n",
    "\n",
    "You can compute the accuracy by computing the count when the label are correctly classified over the total number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8192746536267319"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "cm.filter(cm.income == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can wrap everything together and write a function to compute the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model accuracy: 81.927%\n"
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"income\", \"prediction\")\n",
    "    acc = cm.filter(cm.income == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC metrics\n",
    "\n",
    "The module BinaryClassificationEvaluator includes the ROC measures. The Receiver Operating Characteristic curve is another common tool used with binary classification. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve shows the true positive rate (i.e. recall) against the false positive rate. The false positive rate is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate. The true negative rate is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 - specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.8950386210140153\nareaUnderROC\n"
    }
   ],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='income', rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6) Tune the hyperparameter\n",
    "\n",
    "Last but not least, you can tune the hyperparameters. Similar to scikit learn you create a parameter grid, and you add the parameters you want to tune. To reduce the time of the computation, you only tune the regularization parameter with only two values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you evaluate the model with using the cross valiation method with 5 folds. It takes around 16 minutes to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time to train model: 413.322 seconds\n"
    }
   ],
   "source": [
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model accuracy: 84.658%\n"
    }
   ],
   "source": [
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can exctract the recommended parameter by chaining cvModel.bestModel with extractParamMap() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{Param(parent='LogisticRegression_ffc9dc059fc1', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='featuresCol', doc='features column name.'): 'features',\n Param(parent='LogisticRegression_ffc9dc059fc1', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='labelCol', doc='label column name.'): 'income',\n Param(parent='LogisticRegression_ffc9dc059fc1', name='predictionCol', doc='prediction column name.'): 'prediction',\n Param(parent='LogisticRegression_ffc9dc059fc1', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n Param(parent='LogisticRegression_ffc9dc059fc1', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n Param(parent='LogisticRegression_ffc9dc059fc1', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n Param(parent='LogisticRegression_ffc9dc059fc1', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n Param(parent='LogisticRegression_ffc9dc059fc1', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Spark is a fundamental tool for a data scientist. It allows the practitioner to connect an app to different data sources, perform data analysis seamlessly or add a predictive model.\n",
    "\n",
    "To begin with Spark, you need to initiate a Spark Context with:\n",
    "\n",
    "`SparkContext()``\n",
    "\n",
    "and and SQL context to connect to a data source:\n",
    "\n",
    "`SQLContext()``\n",
    "\n",
    "In the tutorial, you learn how to train a logistic regression:\n",
    "\n",
    "    Convert the dataset to a Dataframe with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))\n",
    "sqlContext.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the label's column name is newlabel and all the features are gather in features. Change these values if different in your dataset.\n",
    "\n",
    "    Create the train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression(labelCol=\"income\",featuresCol=\"features\",maxIter=10, regParam=0.3)\n",
    "lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearModel.transform()"
   ]
  }
 ]
}