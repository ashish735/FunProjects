{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py"},"colab":{"name":"Basic_layout.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"}},"cells":[{"cell_type":"code","metadata":{"id":"-BQm4975_3vv","colab_type":"code","outputId":"41ec13a6-a0e7-4c29-c958-016f8ad8b084","executionInfo":{"status":"ok","timestamp":1589349905157,"user_tz":-330,"elapsed":30714,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":74}},"source":["! pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-563c9958-241d-4d70-a443-e4cf538f8662\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-563c9958-241d-4d70-a443-e4cf538f8662\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving kaggle.json to kaggle.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IiUO9JOy3_8G","colab_type":"code","outputId":"89e66b36-3d15-4686-f223-5682bb669ae2","executionInfo":{"status":"ok","timestamp":1589531426784,"user_tz":-330,"elapsed":7429,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}},"colab":{"base_uri":"https://localhost:8080/","height":483}},"source":["! wget https://www.dropbox.com/s/7mwk6671aa7cckg/simplewiki-20170201-pages-articles-multistream.zip?dl=0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-15 08:30:20--  https://www.dropbox.com/s/7mwk6671aa7cckg/simplewiki-20170201-pages-articles-multistream.zip?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6018:1::a27d:301\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/7mwk6671aa7cckg/simplewiki-20170201-pages-articles-multistream.zip [following]\n","--2020-05-15 08:30:20--  https://www.dropbox.com/s/raw/7mwk6671aa7cckg/simplewiki-20170201-pages-articles-multistream.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com/cd/0/inline/A3wXmJ6C3ZU8ulrQlytgP-pSt2WETokgQXtmGmGZxVR4vnV4Af8R4BMwjW1RZz2RmR15b0ScQj2BoN7UbCXdgD8zOSFjqoYVF_-X4XnePxJo0OsnDdREN8nihvIuZq_bDeQ/file# [following]\n","--2020-05-15 08:30:21--  https://uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com/cd/0/inline/A3wXmJ6C3ZU8ulrQlytgP-pSt2WETokgQXtmGmGZxVR4vnV4Af8R4BMwjW1RZz2RmR15b0ScQj2BoN7UbCXdgD8zOSFjqoYVF_-X4XnePxJo0OsnDdREN8nihvIuZq_bDeQ/file\n","Resolving uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com (uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com)... 162.125.3.6, 2620:100:6018:6::a27d:306\n","Connecting to uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com (uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com)|162.125.3.6|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: /cd/0/inline2/A3yj7sgdaCqRt1lqQpNGDC-acS6IsdIVFIk8YhkCdaqHGoo0_sdV8u04HlFt7ZhhH_UJIwtEuunmsxU9LnkerE7F8nVloi-js_poqGWchAtJ19__UkVtAtH7xkcTTwuTW_ilRZ9bHgH_j5Fxe65z40kW1AyF4QM8BcU4yFxFbQErD0bS2f_xAeToDu3e5wNNR8bmeAFuutzr0_hzTjlQmC3X8G-jhUvHm7r2M__LC_uNt35AOtrQiDfpjVnk6Rc5vm0I6d2QgNwWw9TJznxYWL3eDhGN3MJWrWZuIKRCb1pzggkD6UC7-5W_j9BxZ3a9QbzIpztUVJIcfZgNz1VofTvgOSi0opVQKyCj3X0MBi1xtA/file [following]\n","--2020-05-15 08:30:21--  https://uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com/cd/0/inline2/A3yj7sgdaCqRt1lqQpNGDC-acS6IsdIVFIk8YhkCdaqHGoo0_sdV8u04HlFt7ZhhH_UJIwtEuunmsxU9LnkerE7F8nVloi-js_poqGWchAtJ19__UkVtAtH7xkcTTwuTW_ilRZ9bHgH_j5Fxe65z40kW1AyF4QM8BcU4yFxFbQErD0bS2f_xAeToDu3e5wNNR8bmeAFuutzr0_hzTjlQmC3X8G-jhUvHm7r2M__LC_uNt35AOtrQiDfpjVnk6Rc5vm0I6d2QgNwWw9TJznxYWL3eDhGN3MJWrWZuIKRCb1pzggkD6UC7-5W_j9BxZ3a9QbzIpztUVJIcfZgNz1VofTvgOSi0opVQKyCj3X0MBi1xtA/file\n","Reusing existing connection to uc33743e5fdc331583f814df56c3.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 152787768 (146M) [application/zip]\n","Saving to: ‘simplewiki-20170201-pages-articles-multistream.zip?dl=0’\n","\n","simplewiki-20170201 100%[===================>] 145.71M  44.9MB/s    in 3.2s    \n","\n","2020-05-15 08:30:25 (44.9 MB/s) - ‘simplewiki-20170201-pages-articles-multistream.zip?dl=0’ saved [152787768/152787768]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g75UN_dbWHv-","colab_type":"code","outputId":"d83667b1-471e-48a9-f82c-2142a16ffeda","executionInfo":{"status":"ok","timestamp":1589531452494,"user_tz":-330,"elapsed":8226,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["! unzip '/content/data.zip'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Archive:  /content/data.zip\n","  inflating: simplewiki-20170201-pages-articles-multistream-index.txt  \n","  inflating: simplewiki-20170201-pages-articles-multistream.xml  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-2Os4FbwYFz0","colab_type":"text"},"source":["https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/"]},{"cell_type":"markdown","metadata":{"id":"3GYJjn7nXnJd","colab_type":"text"},"source":["A huge number of text articles are generated everyday from different publishing houses, blogs, media, etc. This leads to one of the major tasks in natural language processing i.e. effectively managing, searching and categorizing articles depending upon their subjects or themes. Typically, these text mining tasks will include text clustering, document similarity and categorization of text. Comprehensively, we have to find out some ways so that the theme of the article can be extracted. In text analytics, this is known as “Topic Modelling”. Also, given a topic, our software should be able to find out articles which are similar to it. This is known as “Document Similarity”."]},{"cell_type":"markdown","metadata":{"id":"KylAeXeTXqid","colab_type":"text"},"source":["Deriving such meaningful information from text documents is the main objective of this blog-post series. I will be covering the whole application of topic modelling in 3 blog-posts. The purpose of the blog-post series is to build the system from scratch and provide an insight of implementation of the same to our readers. This particular post will be focusing on creating a corpus of Simple Wikipedia articles from dumped simple wiki XML file. Once the text data (articles) has been retrieved, it can be used by machine learning techniques for model training in order to discover topics from the text corpus."]},{"cell_type":"markdown","metadata":{"id":"Q8vr4CvsXul3","colab_type":"text"},"source":["There are mainly two steps in the text data retrieval process from simple Wikipedia dump:\n","\n","1. XML parsing of the wiki dump\n","2. Cleaning of the articles’ text"]},{"cell_type":"markdown","metadata":{"id":"-rbFQ4w5X6yu","colab_type":"text"},"source":["The Simple Wikipedia is an edition of the online encyclopedia Wikipedia, primarily written in Basic English. The articles on Simple Wikipedia are usually shorter than their English Wikipedia counterparts, presenting only the basic information. It contains over 127,000 content pages for people to search, explore or even edit. We downloaded the free backup XML file in which all the articles are dumped. Then a sample of 60,000 simple Wikipedia articles is randomly selected for building the application. You can download the same backup XML file(used in this blog) from here or it can be downloaded from index of simple wiki website."]},{"cell_type":"markdown","metadata":{"id":"n0JUcMLLYTZr","colab_type":"text"},"source":["Seeing all this, one can observe that we have to get article text from the text tag in the XML file, which is one of the children of the revision tag (revision itself being a child of the page tag). We will use the Element Tree XML API for parsing the XML file and extracting the text portion of the article. The below Python code traverses down the tree to get the content of the text tag. The contents of each article are extracted from the text tag of that corresponding page in iterations and can be written in separate text files."]},{"cell_type":"markdown","metadata":{"id":"ZInzsOfqYUyF","colab_type":"text"},"source":["## 2. Cleaning of Article Text\n","\n","Data pre-processing (a.k.a data cleaning) is one of the most significant step in text analytics. The purpose is to remove any unwanted words or characters which are written for human readability, but won’t contribute to topic modelling in any way.\n","\n","There are mainly two steps that need to be done on word level:\n","\n","a) Removal of stop words – Stop words like “and”, “if”, “the”, etc are very common in all English sentences and are not very meaningful in deciding the theme of the article, so these words have been removed from the articles.\n","\n","b) Lemmatization – It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider the meaning of the sentence)."]},{"cell_type":"code","metadata":{"id":"N94OESHNZjx2","colab_type":"code","colab":{}},"source":["import xml.etree.ElementTree as ET\n","import codecs\n","import re\n","\n","def is_ascii(s):\n","    return all(ord(c) < 128 for c in s)\n","\n","tree = ET.parse('/content/simplewiki-20170201-pages-articles-multistream.xml')\n","root = tree.getroot()\n","dir_path = '/content/articles-corpus/'\n","\n","for i,page in enumerate(root.findall('{http://www.mediawiki.org/xml/export-0.10/}page')):\n","    for p in page:    \n","        if p.tag == \"{http://www.mediawiki.org/xml/export-0.10/}revision\":\n","            for x in p:\n","                if x.tag == \"{http://www.mediawiki.org/xml/export-0.10/}text\":                    \n","                    article_txt = x.text\n","                    if not article_txt == None:                                                \n","                        article_txt = article_txt[ : article_txt.find(\"==\")]\n","                        article_txt = re.sub(r\"{{.*}}\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\[\\[File:.*\\]\\]\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\[\\[Image:.*\\]\\]\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\n: \\'\\'.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\n!.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"^:\\'\\'.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"&nbsp\",\"\",article_txt)\n","                        article_txt = re.sub(r\"http\\S+\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\d+\",\"\",article_txt)   \n","                        article_txt = re.sub(r\"\\(.*\\)\",\"\",article_txt)\n","                        article_txt = re.sub(r\"Category:.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\| .*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\n\\|.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\n \\|.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\".* \\|\\n\",\"\",article_txt)\n","                        article_txt = re.sub(r\".*\\|\\n\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{Infobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{infobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{taxobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{Taxobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{ Infobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{ infobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{ taxobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"{{ Taxobox.*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\* .*\",\"\",article_txt)\n","                        article_txt = re.sub(r\"<.*>\",\"\",article_txt)\n","                        article_txt = re.sub(r\"\\n\",\"\",article_txt)  \n","                        article_txt = re.sub(r\"\\!|\\\"|\\#|\\$|\\%|\\&|\\'|\\(|\\)|\\*|\\+|\\,|\\-|\\.|\\/|\\:|\\;|\\<|\\=|\\>|\\?|\\@|\\[|\\\\|\\]|\\^|\\_|\\`|\\{|\\||\\}|\\~\",\" \",article_txt)\n","                        article_txt = re.sub(r\" +\",\" \",article_txt)\n","                        article_txt = article_txt.replace(u'\\xa0', u' ')\n","                       \n","                        if not article_txt == None and not article_txt == \"\" and len(article_txt) > 150 and is_ascii(article_txt):\n","                            outfile = dir_path + str(i+1) +\"_article.txt\"\n","                            f = codecs.open(outfile, \"w\", \"utf-8\")\n","                            f.write(article_txt)\n","                            f.close()\n","                            #print article_txt\n","                            #print '\\n=================================================================\\n'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExXpma6gdjMH","colab_type":"text"},"source":["The above code snippet of text filters can be plugged to the text extracted from the text tag (Figure 1). Finally, we keep only those articles which have length more than 150 characters. Also, we check and write only those text articles which contain only ASCII characters (English characters only).\n","\n","This completes the first step towards Topic modeling, i.e. creating the corpus of articles from simple Wikipedia. Once you follow this blog till here, you will be able to create a corpus of around 70,000 articles in the directory “articles-corpus” used in python program. I will be writing about discovering the hidden topics from the corpus created in the next blog-post soon. So stay tuned till then !!"]},{"cell_type":"markdown","metadata":{"id":"4GKejroFdxEb","colab_type":"text"},"source":["## Topic Modelling (Part 2): Discovering Topics from Articles with Latent Dirichlet Allocation"]},{"cell_type":"markdown","metadata":{"id":"w5T71zOpeLnW","colab_type":"text"},"source":["This blog-post is second in the series of blog-posts covering “Topic Modelling” from simple Wikipedia articles. Before reading this post, I would suggest reading our first article here. In the first step towards Topic modeling which entailed creating a corpus of articles from simple Wikipedia, we were able to create a corpus of around 70,000 articles in the directory “articles-corpus”.\n","\n","Look at the above featured image of this blog-post –  these are some of the topics (word distributions) which are the outcome of the experiment undertaken in this post. Lets get started with discovering topics from the corpus of wiki articles. We will be using an unsupervised machine learning technique, Latent Dirichlet Allocation (LDA), for automatically finding the mixture of similar words together, thus forming the topic or theme. From such a huge corpus of articles, we do not have the information about the categories to which these articles belong to or are related. This forms an unsupervised problem where we do not know the labels/classes/categories of the data and aim to find the groups or the clusters within the population. Having said that, I am now going to list down the steps which we have to perform in order to discover the topics hidden in the 60,000 articles, serving as training data:"]},{"cell_type":"markdown","metadata":{"id":"QUhDosoyePZg","colab_type":"text"},"source":["\n","    Pre-processing and training corpus creation\n","    Building dictionary\n","    Feature extraction\n","    LDA model training\n"]},{"cell_type":"markdown","metadata":{"id":"YLZoeNFCecTj","colab_type":"text"},"source":["### 1.  Preprocessing & Training data preparation.\n","\n","As discussed in Part-I, we need to remove the stop words from the articles because they do not contribute to the theme of the article’s content. Similarly, stemming or lemmatization is an effective process in order to treat various inflected forms of words as a single word as they essentially mean the same. I would encourage you to go through the previous post (Part-1) if the above sentences do not make sense to you."]},{"cell_type":"code","metadata":{"id":"hds_TWYUtNpK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"16d54038-4260-43d8-df90-3131bbe0b6c2","executionInfo":{"status":"ok","timestamp":1589535755478,"user_tz":-330,"elapsed":1141,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","import string"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-fUIAit4eQCf","colab_type":"code","colab":{}},"source":["import os\n","import random\n","import codecs\n","import pickle\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n"," \n","# Function to remove stop words from sentences & lemmatize words.\n","def clean(doc):\n","    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n","    normalized = \" \".join(lemma.lemmatize(word,'v') for word in stop_free.split())\n","    x = normalized.split()\n","    y = [s for s in x if len(s) > 2]\n","    return y\n"," \n","# Remember this folder contains 72,000 articles extracted in Part-1 (previous post)\n","corpus_path = \"/content/articles-corpus/\"\n","article_paths = [os.path.join(corpus_path,p) for p in os.listdir(corpus_path)]\n"," \n","# Read contents of all the articles in a list \"doc_complete\"\n","doc_complete = []\n","for path in article_paths:\n","    fp = codecs.open(path,'r','utf-8')\n","    doc_content = fp.read()\n","    doc_complete.append(doc_content)  \n"," \n","# Randomly sample 70000 articles from the corpus created from wiki_parser.py\n","docs_all = random.sample(doc_complete, 70000)\n","docs = open(\"docs_wiki.pkl\",'wb')\n","pickle.dump(docs_all,docs)\n"," \n","# Use 60000 articles for training.\n","docs_train = docs_all[:60000]\n"," \n","# Cleaning all the 60,000 simplewiki articles\n","stop = set(stopwords.words('english'))\n","exclude = set(string.punctuation)\n","lemma = WordNetLemmatizer()\n","doc_clean = [clean(doc) for doc in docs_train]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aln6A2h0fGSr","colab_type":"text"},"source":["In the above code, we are reading all the articles in a list and creating the training data by choosing 60,000 articles from randomly sampled 70,000 articles from that list. The remaining 10,000 articles are left for test purpose (document clustering/categorization) in Part-3. Further, the articles are cleaned by removing stop words and passing each word of corpus through “WordNetLemmatizer”. As a result, we get cleaned articles on which we can build the dictionary and train the LDA model for topic modelling."]},{"cell_type":"code","metadata":{"id":"_CjKuZ9wfN3c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":687},"outputId":"9274680e-fb0b-4449-edc9-01394b34eb97","executionInfo":{"status":"ok","timestamp":1589539879075,"user_tz":-330,"elapsed":1207,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["doc_clean[0]"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['francis',\n"," 'newton',\n"," 'frank',\n"," 'gifford',\n"," 'american',\n"," 'football',\n"," 'player',\n"," 'television',\n"," 'sportscaster',\n"," 'know',\n"," 'broadcast',\n"," 'monday',\n"," 'night',\n"," 'football',\n"," 'gifford',\n"," 'play',\n"," 'new',\n"," 'york',\n"," 'giants',\n"," 'bear',\n"," 'santa',\n"," 'monica',\n"," 'california',\n"," 'august',\n"," 'gifford',\n"," 'die',\n"," 'heart',\n"," 'attack',\n"," 'home',\n"," 'greenwich',\n"," 'connecticut',\n"," 'age',\n"," 'suffer',\n"," 'chronic',\n"," 'traumatic',\n"," 'encephalopathy']"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"URQVA-VGuPbU","colab_type":"text"},"source":["## 2.  Building word dictionary\n","\n","In this step, we need to build the vocabulary of the corpus in which all the unique words of the article corpus are given IDs and their frequency counts are also stored. The following Python code creates the dictionary from the 60,000 randomly sampled cleaned articles. You may note that we are using gensim library for building the dictionary. In gensim, the words are referred to as “tokens” and the index of each word in the dictionary is called “id”."]},{"cell_type":"code","metadata":{"id":"5WA8bN2jt-B3","colab_type":"code","colab":{}},"source":["from gensim import corpora\n","# Creating term dictionary of corpus, where each unique term is assigned an index.\n","dictionary = corpora.Dictionary(doc_clean)\n"," \n","# Filter terms which occurs in less than 4 articles & more than 40% of the articles\n","dictionary.filter_extremes(no_below=4, no_above=0.4)\n"," \n","# List of few words which are removed from dictionary as they are content neutral\n","stoplist = set('also use make people know many call include part find become like mean often different \\\n","               usually take wikt come give well get since type list say change see refer actually iii \\\n","               aisne kinds pas ask would way something need things want every str'.split())\n","stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n","dictionary.filter_tokens(stop_ids)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIeKeItAuaEQ","colab_type":"text"},"source":["Also, it can be seen that there are 2 additional steps performed after creating the dictionary:\n","\n","  All the tokens in the dictionary which either have occurred in less than 4 articles or have occurred in more than 40% of the articles are removed from the dictionary, as these words will not be contributing to the various themes or topics.\n","\n","  After printing the most frequent words of the dictionary, we found that few words which are mostly content neutral words are also present in the dictionary. These words may lead to modeling of “word distribution”(topic) which is neutral and do not capture any theme or content. We made a list of such words and filtered all such words.\n","\n","Once you have built the dictionary, you may find the most frequent words with their respective frequencies like this:"]},{"cell_type":"code","metadata":{"id":"31W8f-8lunfB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"5b0d47a4-d603-423f-b99c-ffddbef6dab6","executionInfo":{"status":"ok","timestamp":1589539960281,"user_tz":-330,"elapsed":930,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["print(dictionary)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Dictionary(26530 unique tokens: ['age', 'american', 'attack', 'august', 'bear']...)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"keArVzomvwfD","colab_type":"text"},"source":["Each word is also given a unique id in the vocabulary (dictionary)."]},{"cell_type":"markdown","metadata":{"id":"uPtRIEg0wBwF","colab_type":"text"},"source":["## 3.  Feature Extraction (Bag of Words)\n","\n","Histograms of words are the features used for text representation. In general, we first build the vocabulary of the article corpus and then we generate a word count vector for each article, which is nothing but the frequencies of all the words in the vocabulary for that particular article. Most of them will be zero as a single article won’t contain all the words in the vocabulary. For example, suppose we have 500 words in vocabulary. So, each word count vector will contain the frequencies of these 500 vocabulary words in a particular wiki article. Suppose that the text in an article was “Get the work done, work done”. So, a fixed length encoding will be generated as [0,0,0,0,0,…….0,0,2,0,0,0,……,0,0,1,0,0,…0,0,1,0,0,……2,0,0,0,0,0]. Here, all the word counts are placed at 296th, 359th, 415th, 495th index of the 500 length word count vector and the rest are zero."]},{"cell_type":"code","metadata":{"id":"LEg5-mLevIIp","colab_type":"code","colab":{}},"source":["# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n","doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwJpsP6wwemV","colab_type":"text"},"source":["The above Python code uses gensim to convert all the 60,000 articles into a document term matrix (word count vector for each document)."]},{"cell_type":"markdown","metadata":{"id":"lJ-j-I35w6rt","colab_type":"text"},"source":["## 4. LDA Model Training\n","\n","We have finally arrived at the training phase of topic modeling. Latent Dirichlet Allocation is an unsupervised probabilistic model which is used to discover latent themes in a document. Let’s try to understand briefly the working of LDA technique.\n","\n","LDA technique makes the following two assumptions:\n","1. Articles/Documents are produced from a mixture of topics. Each article belongs to each of the topics to a certain degree (Each articles is made up of some topic distribution).\n","2. Each topic is a generative model which generates words of the vocabulary with certain probabilities. Words frequently occurring together will have more probability (Each topic is made of some word distribution).\n","\n","So, can you guess the input to this algorithm?\n","\n","Input is the “document-term matrix” which keeps the histograms of words (word count) present in each wiki article. The dimensions of the matrix is (M,N) ,i.e. number of documents * number of words in vocabulary. Documents and articles are interchangeable words here. We also provide K as an input, which is the number of topics that have to be discovered."]},{"cell_type":"markdown","metadata":{"id":"6oFZ6tNexLYp","colab_type":"text"},"source":["What is the output of the Latent Dirichlet Allocation algorithm?\n","\n","The output of LDA algorithm are 2 smaller matrices – a document to topic matrix and a topic to word matrix. Document-Topic matrix is of (M,K) dimensions where M is number of articles andK is number of topics in the vocabulary. Topic-Word matrix is of (K,N) where N is the number of words in the vocabulary."]},{"cell_type":"markdown","metadata":{"id":"Ww92EQOjxiJZ","colab_type":"text"},"source":["Document-Topic matrix accounts for the probability distribution of the topics present in the article. Similarly, Topic-Words matrix accounts for the probability distribution of words that they have been generated from that topic. Both these matrices are initialized randomly and then these distributions are improved upon in an iterative process. After repeating the previous step a large number of times, you’ll eventually reach an approximately steady state where these distributions seem logically correct.\n","\n","The following Python code runs LDA algorithm using gensim implementation. Once the training is completed, the model is dumped using cPickle library for future use and all the 50 topics (learned by model) are printed."]},{"cell_type":"code","metadata":{"id":"WUikY-fOxhSK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"fa6f3377-8d1a-44bf-e3f7-612531b47e79","executionInfo":{"status":"ok","timestamp":1589541458472,"user_tz":-330,"elapsed":504881,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["from gensim.models.ldamodel import LdaModel as Lda\n","# Creating the object for LDA model using gensim library & Training LDA model on the document term matrix.\n","ldamodel = Lda(doc_term_matrix, num_topics=50, id2word = dictionary, passes=50, iterations=500)\n"," \n","# dump LDA model using cPickle for future use\n","ldafile = open('lda_model_sym_wiki.pkl','wb')\n","pickle.dump(ldamodel,ldafile)\n","ldafile.close()\n"," \n","# Print all the 50 topics\n","for i,topic in enumerate(ldamodel.print_topics(num_topics=50, num_words=10)):\n","   words = topic[1].split(\"+\")\n","   print(words,\"\\n\")"],"execution_count":42,"outputs":[{"output_type":"stream","text":["['0.265*\"france\" ', ' 0.118*\"commune\" ', ' 0.084*\"region\" ', ' 0.073*\"department\" ', ' 0.064*\"regions\" ', ' 0.059*\"departments\" ', ' 0.034*\"north\" ', ' 0.027*\"calvados\" ', ' 0.026*\"calais\" ', ' 0.017*\"northwest\"'] \n","\n","['0.053*\"minister\" ', ' 0.040*\"prime\" ', ' 0.036*\"court\" ', ' 0.034*\"roman\" ', ' 0.028*\"carolina\" ', ' 0.027*\"italian\" ', ' 0.021*\"rome\" ', ' 0.020*\"movement\" ', ' 0.018*\"north\" ', ' 0.017*\"wild\"'] \n","\n","['0.045*\"build\" ', ' 0.030*\"air\" ', ' 0.029*\"force\" ', ' 0.020*\"fire\" ', ' 0.019*\"airport\" ', ' 0.018*\"attack\" ', ' 0.017*\"ship\" ', ' 0.015*\"fly\" ', ' 0.015*\"aircraft\" ', ' 0.014*\"military\"'] \n","\n","['0.053*\"color\" ', ' 0.040*\"red\" ', ' 0.037*\"image\" ', ' 0.033*\"blue\" ', ' 0.021*\"world\" ', ' 0.020*\"jpg\" ', ' 0.019*\"file\" ', ' 0.018*\"colour\" ', ' 0.017*\"heritage\" ', ' 0.016*\"map\"'] \n","\n","['0.192*\"movie\" ', ' 0.039*\"play\" ', ' 0.034*\"star\" ', ' 0.034*\"direct\" ', ' 0.031*\"release\" ', ' 0.029*\"director\" ', ' 0.021*\"review\" ', ' 0.020*\"american\" ', ' 0.020*\"comedy\" ', ' 0.016*\"drama\"'] \n","\n","['0.114*\"county\" ', ' 0.060*\"state\" ', ' 0.056*\"california\" ', ' 0.029*\"los\" ', ' 0.028*\"angeles\" ', ' 0.027*\"unite\" ', ' 0.026*\"florida\" ', ' 0.024*\"san\" ', ' 0.017*\"seat\" ', ' 0.017*\"city\"'] \n","\n","['0.052*\"award\" ', ' 0.049*\"television\" ', ' 0.049*\"american\" ', ' 0.036*\"show\" ', ' 0.027*\"americans\" ', ' 0.025*\"best\" ', ' 0.023*\"actor\" ', ' 0.020*\"series\" ', ' 0.018*\"film\" ', ' 0.018*\"bear\"'] \n","\n","['0.081*\"band\" ', ' 0.076*\"album\" ', ' 0.074*\"rock\" ', ' 0.061*\"release\" ', ' 0.026*\"studio\" ', ' 0.020*\"billboard\" ', ' 0.018*\"first\" ', ' 0.017*\"label\" ', ' 0.016*\"metal\" ', ' 0.015*\"software\"'] \n","\n","['0.045*\"division\" ', ' 0.041*\"battle\" ', ' 0.034*\"car\" ', ' 0.032*\"victoria\" ', ' 0.022*\"row\" ', ' 0.022*\"port\" ', ' 0.017*\"fruit\" ', ' 0.015*\"motor\" ', ' 0.015*\"engine\" ', ' 0.015*\"wheel\"'] \n","\n","['0.039*\"blood\" ', ' 0.036*\"heart\" ', ' 0.036*\"disease\" ', ' 0.029*\"wisconsin\" ', ' 0.021*\"columbia\" ', ' 0.018*\"pressure\" ', ' 0.012*\"whose\" ', ' 0.012*\"lincoln\" ', ' 0.011*\"fat\" ', ' 0.010*\"houston\"'] \n","\n","['0.050*\"race\" ', ' 0.042*\"world\" ', ' 0.028*\"wrestle\" ', ' 0.025*\"professional\" ', ' 0.025*\"wwe\" ', ' 0.025*\"championship\" ', ' 0.022*\"hold\" ', ' 0.022*\"one\" ', ' 0.021*\"title\" ', ' 0.020*\"event\"'] \n","\n","['0.026*\"human\" ', ' 0.026*\"cause\" ', ' 0.025*\"body\" ', ' 0.020*\"switzerland\" ', ' 0.019*\"women\" ', ' 0.018*\"child\" ', ' 0.017*\"municipality\" ', ' 0.014*\"canton\" ', ' 0.013*\"female\" ', ' 0.012*\"sex\"'] \n","\n","['0.064*\"company\" ', ' 0.043*\"plant\" ', ' 0.036*\"food\" ', ' 0.020*\"grow\" ', ' 0.019*\"store\" ', ' 0.019*\"flower\" ', ' 0.017*\"own\" ', ' 0.016*\"operate\" ', ' 0.015*\"business\" ', ' 0.013*\"rank\"'] \n","\n","['0.029*\"church\" ', ' 0.029*\"king\" ', ' 0.025*\"england\" ', ' 0.021*\"british\" ', ' 0.015*\"great\" ', ' 0.014*\"queen\" ', ' 0.014*\"royal\" ', ' 0.014*\"john\" ', ' 0.014*\"kingdom\" ', ' 0.013*\"english\"'] \n","\n","['0.033*\"data\" ', ' 0.029*\"ifexpr\" ', ' 0.022*\"channel\" ', ' 0.020*\"news\" ', ' 0.020*\"network\" ', ' 0.020*\"point\" ', ' 0.019*\"display\" ', ' 0.019*\"information\" ', ' 0.018*\"signal\" ', ' 0.016*\"speed\"'] \n","\n","['0.036*\"chemical\" ', ' 0.032*\"category\" ', ' 0.021*\"contain\" ', ' 0.019*\"compound\" ', ' 0.018*\"remove\" ', ' 0.016*\"recognize\" ', ' 0.015*\"burn\" ', ' 0.014*\"committee\" ', ' 0.014*\"acid\" ', ' 0.014*\"reaction\"'] \n","\n","['0.092*\"river\" ', ' 0.045*\"water\" ', ' 0.026*\"lake\" ', ' 0.023*\"sea\" ', ' 0.015*\"valley\" ', ' 0.015*\"southwest\" ', ' 0.014*\"flow\" ', ' 0.014*\"oil\" ', ' 0.013*\"west\" ', ' 0.013*\"coast\"'] \n","\n","['0.032*\"disney\" ', ' 0.021*\"animate\" ', ' 0.020*\"medical\" ', ' 0.018*\"adventure\" ', ' 0.017*\"feel\" ', ' 0.016*\"doctor\" ', ' 0.015*\"health\" ', ' 0.015*\"walt\" ', ' 0.015*\"drive\" ', ' 0.014*\"soul\"'] \n","\n","['0.033*\"person\" ', ' 0.025*\"may\" ', ' 0.022*\"one\" ', ' 0.019*\"example\" ', ' 0.018*\"term\" ', ' 0.016*\"word\" ', ' 0.016*\"work\" ', ' 0.014*\"sometimes\" ', ' 0.012*\"another\" ', ' 0.011*\"think\"'] \n","\n","['0.025*\"kill\" ', ' 0.023*\"god\" ', ' 0.023*\"prize\" ', ' 0.019*\"weight\" ', ' 0.018*\"bad\" ', ' 0.018*\"market\" ', ' 0.018*\"believe\" ', ' 0.015*\"eye\" ', ' 0.015*\"problem\" ', ' 0.014*\"tube\"'] \n","\n","['0.043*\"cite\" ', ' 0.040*\"web\" ', ' 0.018*\"metal\" ', ' 0.016*\"local\" ', ' 0.015*\"table\" ', ' 0.013*\"bond\" ', ' 0.012*\"ball\" ', ' 0.012*\"structure\" ', ' 0.011*\"symbol\" ', ' 0.011*\"elements\"'] \n","\n","['0.019*\"tropical\" ', ' 0.018*\"storm\" ', ' 0.017*\"atlantic\" ', ' 0.014*\"ocean\" ', ' 0.013*\"wind\" ', ' 0.012*\"bank\" ', ' 0.011*\"name\" ', ' 0.011*\"form\" ', ' 0.011*\"center\" ', ' 0.011*\"cause\"'] \n","\n","['0.046*\"system\" ', ' 0.043*\"computer\" ', ' 0.043*\"number\" ', ' 0.026*\"program\" ', ' 0.022*\"field\" ', ' 0.022*\"model\" ', ' 0.020*\"space\" ', ' 0.020*\"science\" ', ' 0.018*\"design\" ', ' 0.017*\"test\"'] \n","\n","['0.048*\"star\" ', ' 0.045*\"replace\" ', ' 0.032*\"sell\" ', ' 0.030*\"wear\" ', ' 0.028*\"drug\" ', ' 0.027*\"night\" ', ' 0.025*\"stand\" ', ' 0.024*\"sun\" ', ' 0.022*\"production\" ', ' 0.016*\"rise\"'] \n","\n","['0.037*\"greek\" ', ' 0.031*\"ancient\" ', ' 0.027*\"style\" ', ' 0.026*\"mountain\" ', ' 0.025*\"key\" ', ' 0.018*\"greece\" ', ' 0.015*\"leave\" ', ' 0.015*\"border\" ', ' 0.012*\"range\" ', ' 0.012*\"minor\"'] \n","\n","['0.061*\"class\" ', ' 0.057*\"british\" ', ' 0.048*\"india\" ', ' 0.029*\"rail\" ', ' 0.025*\"indian\" ', ' 0.021*\"unit\" ', ' 0.020*\"dog\" ', ' 0.019*\"train\" ', ' 0.018*\"front\" ', ' 0.014*\"cat\"'] \n","\n","['0.038*\"island\" ', ' 0.035*\"canada\" ', ' 0.026*\"north\" ', ' 0.025*\"america\" ', ' 0.025*\"south\" ', ' 0.022*\"africa\" ', ' 0.022*\"bird\" ', ' 0.021*\"ice\" ', ' 0.021*\"hockey\" ', ' 0.021*\"black\"'] \n","\n","['0.048*\"pakistan\" ', ' 0.044*\"power\" ', ' 0.044*\"district\" ', ' 0.020*\"heat\" ', ' 0.019*\"energy\" ', ' 0.017*\"moon\" ', ' 0.016*\"electric\" ', ' 0.015*\"nuclear\" ', ' 0.014*\"current\" ', ' 0.013*\"punjab\"'] \n","\n","['0.063*\"book\" ', ' 0.044*\"write\" ', ' 0.032*\"character\" ', ' 0.026*\"series\" ', ' 0.025*\"first\" ', ' 0.024*\"publish\" ', ' 0.020*\"story\" ', ' 0.014*\"base\" ', ' 0.013*\"picture\" ', ' 0.012*\"man\"'] \n","\n","['0.111*\"die\" ', ' 0.090*\"bear\" ', ' 0.084*\"age\" ', ' 0.034*\"death\" ', ' 0.030*\"marry\" ', ' 0.025*\"children\" ', ' 0.022*\"cancer\" ', ' 0.020*\"americans\" ', ' 0.020*\"january\" ', ' 0.017*\"years\"'] \n","\n","['0.102*\"university\" ', ' 0.069*\"school\" ', ' 0.044*\"college\" ', ' 0.036*\"study\" ', ' 0.028*\"chicago\" ', ' 0.025*\"china\" ', ' 0.021*\"high\" ', ' 0.018*\"illinois\" ', ' 0.017*\"chinese\" ', ' 0.017*\"students\"'] \n","\n","['0.032*\"time\" ', ' 0.023*\"years\" ', ' 0.020*\"one\" ', ' 0.018*\"two\" ', ' 0.015*\"day\" ', ' 0.011*\"long\" ', ' 0.011*\"eat\" ', ' 0.010*\"first\" ', ' 0.010*\"four\" ', ' 0.010*\"year\"'] \n","\n","['0.088*\"music\" ', ' 0.053*\"song\" ', ' 0.049*\"record\" ', ' 0.035*\"single\" ', ' 0.024*\"group\" ', ' 0.023*\"singer\" ', ' 0.019*\"pop\" ', ' 0.018*\"songs\" ', ' 0.017*\"perform\" ', ' 0.016*\"hit\"'] \n","\n","['0.051*\"species\" ', ' 0.038*\"live\" ', ' 0.035*\"family\" ', ' 0.023*\"small\" ', ' 0.019*\"tree\" ', ' 0.018*\"animals\" ', ' 0.017*\"genus\" ', ' 0.017*\"large\" ', ' 0.015*\"animal\" ', ' 0.015*\"plant\"'] \n","\n","['0.045*\"texas\" ', ' 0.026*\"brown\" ', ' 0.023*\"hill\" ', ' 0.019*\"board\" ', ' 0.019*\"michigan\" ', ' 0.018*\"tower\" ', ' 0.016*\"urban\" ', ' 0.016*\"navbox\" ', ' 0.015*\"palace\" ', ' 0.015*\"minnesota\"'] \n","\n","['0.107*\"game\" ', ' 0.035*\"video\" ', ' 0.022*\"first\" ', ' 0.017*\"release\" ', ' 0.015*\"series\" ', ' 0.014*\"hot\" ', ' 0.012*\"win\" ', ' 0.011*\"hip\" ', ' 0.011*\"reach\" ', ' 0.010*\"nintendo\"'] \n","\n","['0.033*\"sweden\" ', ' 0.022*\"represent\" ', ' 0.021*\"mario\" ', ' 0.020*\"notable\" ', ' 0.020*\"note\" ', ' 0.020*\"swedish\" ', ' 0.018*\"suicide\" ', ' 0.017*\"poet\" ', ' 0.016*\"arm\" ', ' 0.015*\"van\"'] \n","\n","['0.041*\"radio\" ', ' 0.035*\"ireland\" ', ' 0.033*\"green\" ', ' 0.031*\"host\" ', ' 0.021*\"irish\" ', ' 0.021*\"fox\" ', ' 0.020*\"jackson\" ', ' 0.017*\"scottish\" ', ' 0.017*\"michael\" ', ' 0.016*\"premier\"'] \n","\n","['0.029*\"century\" ', ' 0.022*\"art\" ', ' 0.021*\"work\" ', ' 0.021*\"french\" ', ' 0.017*\"paint\" ', ' 0.016*\"history\" ', ' 0.013*\"museum\" ', ' 0.012*\"famous\" ', ' 0.012*\"one\" ', ' 0.012*\"early\"'] \n","\n","['0.048*\"party\" ', ' 0.021*\"house\" ', ' 0.017*\"serve\" ', ' 0.017*\"member\" ', ' 0.016*\"first\" ', ' 0.015*\"election\" ', ' 0.015*\"governor\" ', ' 0.015*\"summer\" ', ' 0.014*\"politician\" ', ' 0.013*\"elect\"'] \n","\n","['0.040*\"wikipedia\" ', ' 0.035*\"page\" ', ' 0.035*\"article\" ', ' 0.028*\"bar\" ', ' 0.024*\"template\" ', ' 0.022*\"link\" ', ' 0.019*\"user\" ', ' 0.017*\"simple\" ', ' 0.017*\"text\" ', ' 0.017*\"talk\"'] \n","\n","['0.044*\"war\" ', ' 0.020*\"world\" ', ' 0.020*\"government\" ', ' 0.020*\"germany\" ', ' 0.019*\"countries\" ', ' 0.017*\"law\" ', ' 0.016*\"german\" ', ' 0.016*\"union\" ', ' 0.016*\"country\" ', ' 0.014*\"empire\"'] \n","\n","['0.020*\"light\" ', ' 0.016*\"object\" ', ' 0.014*\"earth\" ', ' 0.014*\"year\" ', ' 0.013*\"move\" ', ' 0.011*\"common\" ', ' 0.011*\"gas\" ', ' 0.011*\"sound\" ', ' 0.011*\"measure\" ', ' 0.010*\"shape\"'] \n","\n","['0.195*\"state\" ', ' 0.167*\"unite\" ', ' 0.039*\"president\" ', ' 0.034*\"american\" ', ' 0.024*\"kingdom\" ', ' 0.014*\"washington\" ', ' 0.014*\"national\" ', ' 0.011*\"virginia\" ', ' 0.008*\"massachusetts\" ', ' 0.008*\"first\"'] \n","\n","['0.232*\"new\" ', ' 0.079*\"york\" ', ' 0.068*\"australia\" ', ' 0.055*\"south\" ', ' 0.033*\"wales\" ', ' 0.029*\"australian\" ', ' 0.028*\"city\" ', ' 0.023*\"button\" ', ' 0.020*\"jersey\" ', ' 0.018*\"bay\"'] \n","\n","['0.111*\"football\" ', ' 0.086*\"team\" ', ' 0.080*\"play\" ', ' 0.067*\"national\" ', ' 0.055*\"player\" ', ' 0.034*\"association\" ', ' 0.033*\"league\" ', ' 0.028*\"japan\" ', ' 0.021*\"infobox\" ', ' 0.017*\"club\"'] \n","\n","['0.129*\"city\" ', ' 0.045*\"capital\" ', ' 0.040*\"population\" ', ' 0.040*\"area\" ', ' 0.026*\"largest\" ', ' 0.026*\"province\" ', ' 0.023*\"live\" ', ' 0.023*\"census\" ', ' 0.020*\"cities\" ', ' 0.016*\"country\"'] \n","\n","['0.088*\"language\" ', ' 0.064*\"english\" ', ' 0.036*\"name\" ', ' 0.033*\"word\" ', ' 0.028*\"speak\" ', ' 0.026*\"write\" ', ' 0.019*\"latin\" ', ' 0.018*\"instrument\" ', ' 0.017*\"dance\" ', ' 0.012*\"influence\"'] \n","\n","['0.155*\"season\" ', ' 0.097*\"hurricane\" ', ' 0.041*\"cell\" ', ' 0.031*\"cells\" ', ' 0.030*\"alabama\" ', ' 0.024*\"windows\" ', ' 0.024*\"super\" ', ' 0.017*\"wood\" ', ' 0.016*\"microsoft\" ', ' 0.015*\"figure\"'] \n","\n","['0.033*\"station\" ', ' 0.032*\"town\" ', ' 0.031*\"london\" ', ' 0.026*\"park\" ', ' 0.024*\"line\" ', ' 0.023*\"west\" ', ' 0.022*\"east\" ', ' 0.018*\"england\" ', ' 0.016*\"north\" ', ' 0.016*\"south\"'] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zwo1aOGb4av6","colab_type":"text"},"source":["Each row above represents a topic and their word distribution "]},{"cell_type":"markdown","metadata":{"id":"qE5iEfVc4ElU","colab_type":"text"},"source":["Final Thoughts\n","\n","Hope it was an easy task for our readers to follow the blog-post till here. In this post, I have tried to explain the pipeline of the topic discovery process, from preparing the training data to the training of the LDA model. I have also tried to briefly explain the Latent Dirichlet Allocation algorithm to provide an idea of what goes into and what comes out from the LDA model. I would encourage readers to implement this series of blog-posts (see Part-1), and match their outputs with the results shown here (though topics discovered can be different at every run).\n","\n","There are several factors that you can experiment with in order to get even better word distributions forming the topics:\n","\n","1. Getting more number of articles : You can try increasing the number of articles by changing the minimum article length from 150 to 100 characters in Part-1. Also see if we can prevent discarding the articles which contains few non-ASCII characters. More training data may lead to better topic-word distribution.\n","\n","2. Preprocessing : By analyzing the word distributions of topics generated, you may find\n","\n","  pairs that are always juxtaposed (entities) e.g. “Los Angeles” (topic 20), “New York” (topic 27). These pairs should be combined like Los_Angeles or New_York.\n","  \n","  words that are not properly lemmatized like (germany, german), (chinese, china), (america,americans) etc. Lemmatization of nouns may help. Remember, we did lemmatization of verbs lemma.lemmatize(word,'v').\n","\n","3. Dictionary : The vocabulary of the corpus can be improved by removing the content neutral words. Iteratively running the whole topic discovery process and analyzing the word distributions (topics) can help in finding content neutral words from dictionary. Some example are “ing” (topic 10), “per” (Topic 43).\n","\n","4. Parameters of LDA : There are two parameters of LDA to look upon –  alpha and beta. Understanding the mathematics behind LDA model may help in tuning these parameters. I would encourage readers to do so."]},{"cell_type":"markdown","metadata":{"id":"L1XhXM184_qs","colab_type":"text"},"source":["## Topic Modelling (Part 3): Document Clustering, Exploration & Theme Extraction from SimpleWiki Articles"]},{"cell_type":"markdown","metadata":{"id":"EOiENCQG5COE","colab_type":"text"},"source":["In this final blog-post of the “Topic Modelling” series, we will see following usages of the knowledge acquired (the topics discovered) after LDA training.\n","\n","    Document Clustering : Clustering the set of similar wiki-articles in 50 clusters.\n","    Document Exploration : Given some word, search related articles.\n","    Theme Extraction : Find out the theme of the article.\n","\n","As the first step, we will write a function to clean the test article similar to what we had used before training the corpus. The pre-processing is always required before fetching the articles for any  of the above mentioned applications."]},{"cell_type":"code","metadata":{"id":"ItVIhycP5NdX","colab_type":"code","colab":{}},"source":["def rem_ascii(s):\n","    return \"\".join([ch for ch in s if ord(ch) < 128 ])\n","    return y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pLE-rXbc5agq","colab_type":"text"},"source":["In the pre-processing step, we are basically removing the non-ASCII characters, punctuation marks and stop words. Other than that, we are also lemmatizing the words of the input articles."]},{"cell_type":"markdown","metadata":{"id":"E8wF1ya45ftL","colab_type":"text"},"source":["## 1. Document Clustering\n","\n","Document clustering is an unsupervised approach to cluster the articles depending upon the topics which have been discovered in the training phase. Document clustering takes a corpus of unlabeled articles as an input and categorizes them in various groups according to the best matched word distributions (topics) generated from training. Following are the steps performed for document clustering.\n","\n","  Clean all the articles in the input cluster.\n","\n","  Convert each of the text articles into bag-of-words features using the same dictionary of trained model.\n","\n","  Extract the best matched topic from each article using the trained LDA model. In gensim implementation, we have get_document_topic()function which does the same.\n","\n","  Write the article in the directory belonging to extracted topic if minimum probability criteria is satisfied, otherwise push it in the “unknown” directory.\n","  \n","  If the extracted topic (word distribution) is ambiguous , then we choose the 2nd best matched topic (as some of the discovered 50 topics are content neutral).\n"]},{"cell_type":"code","metadata":{"id":"jflO2WwZEzks","colab_type":"code","colab":{}},"source":["def clean_doc(doc):\n","    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n","    normalized = \" \".join(lemma.lemmatize(word,'v') for word in stop_free.split())\n","    x = normalized.split()\n","    y = [s for s in x if len(s) > 2]\n","    return y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CvsaHP35VG3","colab_type":"code","colab":{}},"source":["def cluster_similar_documents(corpus, dirname):\n","    clean_docs = [clean_doc(doc) for doc in corpus]\n","    test_term = [ldamodel.id2word.doc2bow(doc) for doc in clean_docs]\n","    doc_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.20)\n","    for k,topics in enumerate(doc_topics):\n","        if topics:\n","            topics.sort(key = itemgetter(1), reverse=True)\n","            dir_name = dirname + \"/\" + str(topics[0][0])\n","            file_name = dir_name + \"/\" + str(k) + \".txt\"\n","            if not os.path.exists(dir_name):\n","                os.makedirs(dir_name)\n","            fp = open(file_name,\"w\")\n","            fp.write(docs_test[k] + \"\\n\\n\" + str(topics[0][1]) )\n","            fp.close()\n","        else:\n","            if not os.path.exists(dirname + \"/unknown\"):\n","                os.makedirs(dirname + \"/unknown\")\n","            file_name = dirname + \"/unknown/\" + str(k) + \".txt\"\n","            fp = open(file_name,\"w\")\n","            fp.write(docs_test[k])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cerDYKur6JOA","colab_type":"text"},"source":["The above Python function follows the above steps to perform document clustering given an article corpus. It also takes a parameter dirname under which it creates 50 sub-directories containing clustered articles."]},{"cell_type":"markdown","metadata":{"id":"c8rnVxFA6W8H","colab_type":"text"},"source":["2. Document Exploration\n","\n","Document exploration can be another application that can be build over the trained LDA model. Here, given a word or theme, we extract the documents related to it. Mainly, it is a two step process:\n","\n","    Get the best matched topic cluster (highest probability) for the given word.\n","    Get “top” most probable related articles from the matched topic cluster in step 1.\n","\n","get_term_topics() is the function which is used for getting the best matched topic cluster given a theme/word."]},{"cell_type":"code","metadata":{"id":"ZrvLGkdt6Fon","colab_type":"code","colab":{}},"source":["def get_related_documents(term, top, corpus):\n","    clean_docs = [clean_doc(doc) for doc in corpus]\n","    related_docid = []\n","    test_term = [ldamodel.id2word.doc2bow(doc) for doc in clean_docs]\n","    doc_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.20)\n","    term_topics =  ldamodel.get_term_topics(term, minimum_probability=0.000001)\n","    for k,topics in enumerate(doc_topics):\n","        if topics:\n","            topics.sort(key = itemgetter(1), reverse=True)\n","            if topics[0][0] == term_topics[0][0]:\n","                related_docid.append((k,topics[0][1]))\n"," \n","    related_docid.sort(key = itemgetter(1), reverse=True)\n","    for j,doc_id in enumerate(related_docid):\n","        print(doc_id[1],\"\\n\\n\",docs_test[doc_id[0]])\n","        if(j == (top-1)):\n","            break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJHp3R-56lRs","colab_type":"text"},"source":["The above Python function implements a documents exploring system where given a word/theme/topic as an input, it prints the “top” most related articles from the simple wiki test corpus. The test corpus is also given as input to the function."]},{"cell_type":"markdown","metadata":{"id":"3Gd6ER9c622L","colab_type":"text"},"source":["## 3. Theme Extraction\n","\n","We know that 50 word distributions were discovered in Part-2 of this blog-post series. I have manually given theme names to each of the word distributions. You may give different theme names depending upon how you look at the word distributions. If each of the word distributions discovered accurately belong to a particular theme, then topic extraction from articles can be another useful application that can be implemented. You can view this file to see the mapping of manually given topics to the word distributions discovered."]},{"cell_type":"code","metadata":{"id":"_cN5S1hu6azX","colab_type":"code","colab":{}},"source":["def get_theme(doc):\n","    # below topics variable is manually defined by looking at 50 topics created by LDA model\n","    topics = \"Electrical_systems_or_Education unknown music unknown Software International_event Literature War_or_Church Lingual_or_Research Biology Waterbody Wikipedia_or_Icehockey unknown unknown html_tags sports TV_shows Terms_and_Services music US_states Timeline Chemistry Germany Location_area Film_awards Games US_school unknown Railways Biography Directions_Australlia France India_Pakistan Canada_politcs_or_WWE Politics unknown British_Royal_Family American_Movies unknown Colors_or_Birds Fauna Chinese_Military unknown unknown unknown unknown unknown html_tags US_Govt Music_band\".split()\n","    theme = \"\"\n","    cleandoc = clean_doc(doc)\n","    doc_bow = ldamodel.id2word.doc2bow(cleandoc)\n","    doc_topics = ldamodel.get_document_topics(doc_bow, minimum_probability=0.20)\n","    if doc_topics:\n","        doc_topics.sort(key = itemgetter(1), reverse=True)\n","        theme = topics[doc_topics[0][0]]\n","        if theme == \"unknown\":\n","            theme = topics[doc_topics[1][0]]\n","    else:\n","        theme = \"unknown\"\n","    return theme"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jguUOTHW79Ax","colab_type":"text"},"source":["The above Python function extracts the theme from article given as an argument. Having written 3 different functions, we will see now how we can call them in main program. The following Python snippet can be executed to perform these applications"]},{"cell_type":"code","metadata":{"id":"D-z7tirs67i2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":372},"outputId":"f5000471-acae-4a9f-cbda-d9c72351a318","executionInfo":{"status":"ok","timestamp":1589541846605,"user_tz":-330,"elapsed":17201,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import pickle\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from operator import itemgetter\n","import os\n"," \n","# initialize WordNetLemmatizer and get the list of english stop words\n","stop = set(stopwords.words('english'))\n","lemma = WordNetLemmatizer()\n"," \n","# Load trained LDA model (described in Part-2 of blog-post series)\n","lda_fp = open(\"/content/lda_model_sym_wiki.pkl\", 'rb')\n","ldamodel = pickle.load(lda_fp)\n"," \n","# Load the articles corpus to choose 10,000 files for test purpose\n","docs_fp = open(\"/content/docs_wiki.pkl\", 'rb')\n","docs_all = pickle.load(docs_fp)\n","docs_test = docs_all[60000:]\n"," \n","# Get 'top' related documents given a word(term)\n","get_related_documents(\"music\",5,docs_test)\n","# performs document clustering given a set of documents\n","cluster_similar_documents(docs_test,'/content/cluster')\n"," \n","# Extract the theme of article\n","article = \"Mohandas Karamchand Gandhi[14] was born on 2 October 1869[1] to a Hindu Modh Baniya family[15] in Porbandar (also known as Sudamapuri), a coastal town on the Kathiawar Peninsula and then part of the small princely state of Porbandar in the Kathiawar Agency of the Indian Empire. His father, Karamchand Uttamchand Gandhi (1822–1885), served as the diwan (chief minister) of Porbandar state.[16] Although he only had an elementary education and had previously been a clerk in the state administration, Karamchand proved a capable chief minister.[17] During his tenure, Karamchand married four times. His first two wives died young, after each had given birth to a daughter, and his third marriage was childless. In 1857, Karamchand sought his third wife's permission to remarry; that year, he married Putlibai (1844–1891), who also came from Junagadh,[18] and was from a Pranami Vaishnava family.[19][20][21][22] Karamchand and Putlibai had three children over the ensuing decade, a son, Laxmidas (c. 1860 – March 1914), a daughter, Raliatbehn (1862–1960) and another son, Karsandas (c. 1866–1913)\"\n","print(\"For the given article :\", \"\\n\")\n","print(\"Theme -> \",get_theme(article))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["0.6751872 \n","\n","  Emotional Rescue is the tenth Worldwide studio album and seventeenth American studio album by The Rolling Stones The album was released on June It was the group s seventh studio album with label Rolling Stones Records \n","0.64980745 \n","\n","  album ratings Bathory is the first studio album by the Sweden Swedish extreme metal band Bathory Bathory It was released in October through Tyfon Grammofon Many people say that this album is the very first black metal album \n","0.64852554 \n","\n","  Their Satanic Majesties Request is the eighth American studio album and first Worldwide studio album by The Rolling Stones The album was released on December \n","0.64146054 \n","\n","  The World Needs a Hero is the ninth studio album by American thrash metal band Megadeth The World Needs a Hero was released on May through Sanctuary Records Group Sanctuary Records It is Megadeth s last album to feature original bassist David Ellefson until he returned in It is also the only Megadeth album to feature Al Pitrelli on lead guitar and the last of two studio albums to feature drummer Jimmy DeGrasso The album peaked at on the Billboard Billboard and at on the Canadian Albums Chart \n","0.587261 \n","\n","  Slow Dazzle is the fifth studio album by multi instrumentalist John Cale It was released in March It was his second full length album on Island Records label \n","For the given article : \n","\n","Theme ->  Biography\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"feOkZYS59DeP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}