{"cells":[{"cell_type":"code","metadata":{"id":"_iaNwqzbp4HB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"bdf6450b-bf35-4e38-e5d2-d059685ac185","executionInfo":{"status":"ok","timestamp":1589271632641,"user_tz":-330,"elapsed":3985,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["! unzip '/content/trip_advisor_hackerearth_data.zip'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Archive:  /content/trip_advisor_hackerearth_data.zip\n","  inflating: test.csv                \n","  inflating: train.csv               \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GQ1Fxm4hqXCY","colab_type":"text"},"source":["Problem Statement\n","\n","“TripAdvisor is the world’s largest travel site where you can compare and book hotels, flights, restaurants etc. The data set provided in this challenge consists of a sample of hotel reviews provided by the customers. Analyzing customers reviews will help them understand about the hotels listed on their website i.e. if they are treating customers well or if they are providing hospitality services as expected.\n","\n","In this challenge, you have to predict if a customer is happy or not happy.” \n","\n","Data-set Description\n","\n","You are given three files to download: train.csv, test.csv and sample_submission.csv. The training data has 38932 rows, while the test data has 29404 rows. You can download the .csv files from "]},{"cell_type":"markdown","metadata":{"id":"msldqChHrs__","colab_type":"text"},"source":["Variable \tDescription\n","User_ID \tunique ID of the customer\n","Description \tdescription of the review posted\n","Browser_Used \tbrowser used to post the review\n","Device_Used \tdevice used to post the review\n","Is_Response \ttarget variable\n","\n","We are interested only in 2 columns. ‘Description’ which contains hotel reviews given by different users and ‘Is_Response’ which keeps the record of ‘happy’ or ‘not_happy’. So, in essence, this is simply a 2-class sentiment analysis problem.\n","\n","The steps we are going to follow in this blog-post are as follows:\n","\n","    Prepare data\n","    Feature Extraction.\n","    Build the Model.\n","    Train the Model.\n","    Checking Performance.\n"]},{"cell_type":"code","metadata":{"id":"iaGCxUtIsg04","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"c2c13226-99c9-434b-a4a8-78a7cac4f407","executionInfo":{"status":"ok","timestamp":1589267096048,"user_tz":-330,"elapsed":3925,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","import json\n","import keras\n","import keras.preprocessing.text as kpt\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HI-rJ3Ecs08W","colab_type":"code","colab":{}},"source":["train = pd.read_csv('/content/train.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMpfOTK2s7Jw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"c1248671-bd57-4b0a-d1d5-9bb95f9a9a8e","executionInfo":{"status":"ok","timestamp":1589267123530,"user_tz":-330,"elapsed":1000,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["train.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User_ID</th>\n","      <th>Description</th>\n","      <th>Browser_Used</th>\n","      <th>Device_Used</th>\n","      <th>Is_Response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id10326</td>\n","      <td>The room was kind of clean but had a VERY stro...</td>\n","      <td>Edge</td>\n","      <td>Mobile</td>\n","      <td>not happy</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id10327</td>\n","      <td>I stayed at the Crown Plaza April -- - April -...</td>\n","      <td>Internet Explorer</td>\n","      <td>Mobile</td>\n","      <td>not happy</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id10328</td>\n","      <td>I booked this hotel through Hotwire at the low...</td>\n","      <td>Mozilla</td>\n","      <td>Tablet</td>\n","      <td>not happy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>id10329</td>\n","      <td>Stayed here with husband and sons on the way t...</td>\n","      <td>InternetExplorer</td>\n","      <td>Desktop</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>id10330</td>\n","      <td>My girlfriends and I stayed here to celebrate ...</td>\n","      <td>Edge</td>\n","      <td>Tablet</td>\n","      <td>not happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   User_ID  ... Is_Response\n","0  id10326  ...   not happy\n","1  id10327  ...   not happy\n","2  id10328  ...   not happy\n","3  id10329  ...       happy\n","4  id10330  ...   not happy\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"aSJT2Y8jtrnK","colab_type":"text"},"source":["Prepare Data\n","\n","Training data is provided in .csv format which can be ingested easily with pandas as shown in the code below. ‘Is_Response’ field of data carries strings, i.e. ‘happy’ and ‘not_happy’, which needs to be encoded in integer format, i.e. 0 and 1. Here, it is done by LabelEncoder class of scikit-learn library. The function returns list of a hotel reviews and their respective happiness labels."]},{"cell_type":"code","metadata":{"id":"Qg9b9H9GtiPU","colab_type":"code","colab":{}},"source":["def data_prepare(training_file_path):\n","    dataset = pd.read_csv(training_file_path)\n","    reviews = []\n","    labels = []    \n"," \n","    # Enconding Categorical Data\n","    labelencoder_y = LabelEncoder()\n","    dataset['Is_Response'] = labelencoder_y.fit_transform(dataset['Is_Response'])\n","    cLen = len(dataset['Description'])\n"," \n","    for i in range(0,cLen):\n","        review = dataset['Description'][i]\n","        reviews.append(review)\n","        label = dataset[\"Is_Response\"][i]\n","        labels.append(label)\n","    labels = np.asarray(labels)\n","    return reviews,labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7heVnTRzuIDn","colab_type":"text"},"source":["Feature Extraction\n","\n","In this task, words are features, hence the bag-of-words model can be used to create a feature vector. It can be done in following steps:\n","\n","1. Make a dictionary : We create a dictionary containing word-index tuples of all the distinct words in training text reviews. We assume that the ordering of words is not important.\n","\n","2. Convert words of each text review into word index array and store the index array of each review in global array. Example of a text review –"]},{"cell_type":"markdown","metadata":{"id":"aeuixZnfuMRl","colab_type":"text"},"source":["The room was kind of clean but had a VERY strong smell of dogs. Generally below average but ok for a overnight stay if you're not too fussy. Would consider staying again if the price was right. Breakfast was free and just about better than nothing.\n","[1, 14, 5, 436, 9, 52, 17, 25, 3, 22, 1735, 628, 9, 1727, 1109, 943, 492, 17, 322, 11, 3, 1010, 34, 42, 411, 24, 131, 3754, 40, 941, 181, 72, 42, 1, 126, 5, 117, 60, 5, 89, 2, 56, 64, 172, 100, 268]"]},{"cell_type":"markdown","metadata":{"id":"BGJOIyM9uQwj","colab_type":"text"},"source":["Convert the global array of index into a feature matrix. Each text review is represented by a sparse vector of the size of the vocabulary, with 1 in the entries representing the word and 0 in all other entries. We use the maximum number of features as 10,000. Thus the final feature matrix will be of shape (38392,10000)."]},{"cell_type":"code","metadata":{"id":"ZGKQ4hxVuSV8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"3d29c825-ef65-480f-ea62-f5beb3df9356","executionInfo":{"status":"ok","timestamp":1589267484001,"user_tz":-330,"elapsed":1071,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["train.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(38932, 5)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"521x52uVu2oe","colab_type":"text"},"source":["While training the model, we pass the feature matrix, the labels, input batch size to process, the number of iterations etc as parameters . We also save the dictionary and the NN model in order to use them later while performing predictions on the test data. Once the NN model has been trained, we can check the performance of the model on test .csv data."]},{"cell_type":"code","metadata":{"id":"0QjJwi1HvCqo","colab_type":"code","colab":{}},"source":["def convert_text_to_index_array(text):\n","    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBNqpnpBvEPw","colab_type":"code","colab":{}},"source":["train_file_path = \"/content/train.csv\"\n","[reviews,labels] = data_prepare(train_file_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZlpuk8CvUxS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"00628af4-d89f-4213-b2df-c3d7883994ea","executionInfo":{"status":"ok","timestamp":1589267791448,"user_tz":-330,"elapsed":974,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["len(reviews), len(labels)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(38932, 38932)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"kMhZLompvW0y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"d8b85f28-7a26-49ec-8aa0-7307729ecc1b","executionInfo":{"status":"ok","timestamp":1589267834454,"user_tz":-330,"elapsed":1076,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["print(reviews[0])\n","print(labels[0])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["The room was kind of clean but had a VERY strong smell of dogs. Generally below average but ok for a overnight stay if you're not too fussy. Would consider staying again if the price was right. Breakfast was free and just about better than nothing.\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fiqPklL6vqF-","colab_type":"code","colab":{}},"source":["# Create Dictionary of words and their indices\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(reviews)\n","dictionary = tokenizer.word_index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKQhcmcxwcsW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"b077c2ca-aac9-435b-e045-c35bcff85ecd","executionInfo":{"status":"ok","timestamp":1589268134241,"user_tz":-330,"elapsed":1231,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["cnt=0\n","for key, word in dictionary.items():\n","  print(key, word)\n","  cnt+=1\n","  if(cnt==5):\n","    break"],"execution_count":17,"outputs":[{"output_type":"stream","text":["the 1\n","and 2\n","a 3\n","to 4\n","was 5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YbIUVcR1wypX","colab_type":"code","colab":{}},"source":["# save dictionary\n","with open('dictionary.json','w') as dictionary_file:\n","    json.dump(dictionary,dictionary_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"61qzFL2zw58A","colab_type":"code","colab":{}},"source":["# Replace words of each text review to indices\n","allWordIndices = []\n","for num,text in enumerate(reviews):\n","    wordIndices = convert_text_to_index_array(text)\n","    allWordIndices.append(wordIndices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1dZ-fsWGyGCW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":873},"outputId":"114610de-b008-4e3a-9f47-58fa6b3f2768","executionInfo":{"status":"ok","timestamp":1589268480053,"user_tz":-330,"elapsed":1053,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["allWordIndices[0]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1,\n"," 14,\n"," 5,\n"," 436,\n"," 9,\n"," 52,\n"," 17,\n"," 25,\n"," 3,\n"," 22,\n"," 1735,\n"," 628,\n"," 9,\n"," 1727,\n"," 1109,\n"," 943,\n"," 492,\n"," 17,\n"," 322,\n"," 11,\n"," 3,\n"," 1010,\n"," 34,\n"," 42,\n"," 411,\n"," 24,\n"," 131,\n"," 3754,\n"," 40,\n"," 941,\n"," 181,\n"," 72,\n"," 42,\n"," 1,\n"," 126,\n"," 5,\n"," 117,\n"," 60,\n"," 5,\n"," 89,\n"," 2,\n"," 56,\n"," 64,\n"," 172,\n"," 100,\n"," 268]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"2oobEt_XxmTG","colab_type":"code","colab":{}},"source":["# Convert the index sequences into binary bag of words vector (one hot encoding)\n","allWordIndices = np.asarray(allWordIndices)\n","train_X = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n","labels = keras.utils.to_categorical(labels,num_classes=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7ONyNCRyPM_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":147},"outputId":"18c692a5-8e1d-4b57-8fd8-d5d6e719e44c","executionInfo":{"status":"ok","timestamp":1589268537805,"user_tz":-330,"elapsed":1008,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["train_X, train_X.shape"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[0., 1., 1., ..., 0., 0., 0.],\n","        [0., 1., 1., ..., 0., 0., 0.],\n","        [0., 1., 1., ..., 0., 0., 0.],\n","        ...,\n","        [0., 1., 1., ..., 0., 0., 0.],\n","        [0., 1., 1., ..., 0., 0., 0.],\n","        [0., 1., 1., ..., 0., 0., 0.]]), (38932, 10000))"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"N4CgRppFySF8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":147},"outputId":"6e0363f7-6048-4fb3-f759-6388ed32fd22","executionInfo":{"status":"ok","timestamp":1589268552852,"user_tz":-330,"elapsed":1124,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["labels, labels.shape"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[0., 1.],\n","        [0., 1.],\n","        [0., 1.],\n","        ...,\n","        [0., 1.],\n","        [0., 1.],\n","        [1., 0.]], dtype=float32), (38932, 2))"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"174LZQ1fyZVF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":242},"outputId":"76ba4abd-f84c-4f91-fe05-14675e9609d0","executionInfo":{"status":"ok","timestamp":1589268792228,"user_tz":-330,"elapsed":170698,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["# Creating Dense Neural Network Model\n","model = Sequential()\n","model.add(Dense(256, input_shape=(max_words,), activation='elu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(128, activation='elu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(2, activation='softmax'))\n"," \n","model.compile(loss='categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy'])\n"," \n","# Training the Model\n","model.fit(train_X, labels,\n","  batch_size=32,\n","  epochs=5,\n","  verbose=1,\n","  validation_split=0.1,\n","  shuffle=True)\n"," \n","# Save model to disk\n","model_json = model.to_json()\n","with open('model.json', 'w') as json_file:\n","    json_file.write(model_json)\n","model.save_weights('model.h5')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Train on 35038 samples, validate on 3894 samples\n","Epoch 1/5\n","35038/35038 [==============================] - 34s 978us/step - loss: 0.3303 - accuracy: 0.8647 - val_loss: 0.2794 - val_accuracy: 0.8808\n","Epoch 2/5\n","35038/35038 [==============================] - 34s 962us/step - loss: 0.2450 - accuracy: 0.9005 - val_loss: 0.2846 - val_accuracy: 0.8829\n","Epoch 3/5\n","35038/35038 [==============================] - 34s 973us/step - loss: 0.2149 - accuracy: 0.9109 - val_loss: 0.2995 - val_accuracy: 0.8806\n","Epoch 4/5\n","35038/35038 [==============================] - 34s 957us/step - loss: 0.1918 - accuracy: 0.9193 - val_loss: 0.3675 - val_accuracy: 0.8780\n","Epoch 5/5\n","35038/35038 [==============================] - 34s 964us/step - loss: 0.1713 - accuracy: 0.9246 - val_loss: 0.4143 - val_accuracy: 0.8744\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6-2zRljnz6ZK","colab_type":"text"},"source":["Checking Performance\n","\n","In HackerEarth challenge, the test.csv file is provided and it consists of 29404 hotel reviews. We will now predict the sentiment for all the hotel reviews. To find the accuracy (score) of the model, one needs to upload the prediction csv file on the portal here.\n","\n","To check the performance of the “Predict the Happiness” system, the trained dictionary and the NN model is loaded. For each of the hotel reviews, we extract the bag of word features in a similar way as in training. The softmax scores of the output layer are calculated by feedforwarding the input features to the trained NN model. A higher score shows more probability of that sentiment. Finally, the prediction csv file is written with User_ID and the predicted response. The Python code for performing predictions on the test data is shown below."]},{"cell_type":"code","metadata":{"id":"yxPyWZMDyqg2","colab_type":"code","colab":{}},"source":["import json\n","import numpy as np\n","import keras.preprocessing.text as kpt\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import model_from_json\n","import pandas as pd\n"," \n","def convert_text_to_index_array(text):\n","    words = kpt.text_to_word_sequence(text)\n","    wordIndices = []\n","    for word in words:\n","        if word in dictionary:\n","            wordIndices.append(dictionary[word])\n","    return wordIndices\n"," \n","# Load the dictionary\n","labels = ['happy','not_happy']\n","with open('/content/dictionary.json', 'r') as dictionary_file:\n","    dictionary = json.load(dictionary_file)\n"," \n","# Load trained model\n","json_file = open('/content/model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model = model_from_json(loaded_model_json)\n","model.load_weights('/content/model.h5')\n"," \n","testset = pd.read_csv(\"/content/test.csv\")\n","cLen = len(testset['Description'])\n","tokenizer = Tokenizer(num_words=10000)\n"," \n","# Predict happiness for each review in test.csv\n","y_pred = []\n","for i in range(0,cLen):\n","    review = testset['Description'][i]\n","    testArr = convert_text_to_index_array(review)\n","    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n","    pred = model.predict(input)\n","    #print pred[0][np.argmax(pred)] * 100, labels[np.argmax(pred)]\n","    y_pred.append(labels[np.argmax(pred)])\n"," \n","# Write the results in submission csv file\n","raw_data = {'User_ID': testset['User_ID'],\n","        'Is_Response': y_pred}\n","df = pd.DataFrame(raw_data, columns = ['User_ID', 'Is_Response'])\n","df.to_csv('submission_model1.csv', sep=',',index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wa5ksuMn3OLE","colab_type":"text"},"source":["# State-of-the-Art Text Classification using BERT model: “Predict the Happiness” Challenge"]},{"cell_type":"markdown","metadata":{"id":"6_qmXi_l3udr","colab_type":"text"},"source":["Much recently in October, 2018, Google released new language representation model called BERT, which stands for “Bidirectional Encoder Representations from Transformers”. According to their paper, It obtains new state-of-the-art results on wide range of natural language processing tasks like text classification, entity recognition, question and answering system etc."]},{"cell_type":"markdown","metadata":{"id":"XryhqGt63400","colab_type":"text"},"source":["Installation\n","\n","As far as tensorflow based installation are concerned, It is easy to set up the experiment. In your python tensorflow environment, just follow these two steps.\n","\n","    Clone the BERT Github repository onto your own machine. On your terminal, type\n","    git clone https://github.com/google-research/bert.git\n","    Download the pre-trained model from official BERT Github page here. There are 4 types of per-trained models.\n","    BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters\n","    BERT-Large, Uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","    BERT-Base, Cased: 12-layer, 768-hidden, 12-heads , 110M parameters\n","    BERT-Large, Cased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","\n","I downloaded the BERT-Base, Cased one for the experiment as the text data-set used had cased words. Also, base models are only 12 layers deep neural network (as opposed to BERT-Large which is 24 layers deep) which can run on GTX 1080Ti (11 GB VRAM). BERT-Large models can not run on 11 GB GPU memory and it would require more space to run (64GB would suffice)."]},{"cell_type":"code","metadata":{"id":"z-21SgXo3vZ8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":242},"outputId":"2d5ccb76-96ec-4588-ea50-6f4de70ed898","executionInfo":{"status":"ok","timestamp":1589271693568,"user_tz":-330,"elapsed":7734,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-05-12 08:21:27--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.216.128, 2607:f8b0:400c:c07::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.216.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 404261442 (386M) [application/zip]\n","Saving to: ‘cased_L-12_H-768_A-12.zip’\n","\n","cased_L-12_H-768_A- 100%[===================>] 385.53M  90.1MB/s    in 4.3s    \n","\n","2020-05-12 08:21:31 (90.1 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jsbt6P8x56Be","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":147},"outputId":"1a6fcfc5-a8f2-44c8-b44f-2a5b0d475d98","executionInfo":{"status":"ok","timestamp":1589271712515,"user_tz":-330,"elapsed":7861,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["! unzip '/content/cased_L-12_H-768_A-12.zip'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Archive:  /content/cased_L-12_H-768_A-12.zip\n","   creating: cased_L-12_H-768_A-12/\n","  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n","  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n","  inflating: cased_L-12_H-768_A-12/vocab.txt  \n","  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n","  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"],"name":"stdout"}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! git clone https://github.com/google-research/bert.git"]},{"cell_type":"markdown","metadata":{"id":"Z_Vpw4GY6d1J","colab_type":"text"},"source":["Preparing Data for Model\n","\n","We need to prepare the text data in a format that it complies with BERT model. Basically codes written by Google to apply BERT accepts the “Tab separated” file in following format.\n","\n","train.tsv or dev.tsv\n","\n","    an ID for the row\n","    the label for the row as an int (class labels: 0,1,2,3 etc)\n","    A column of all the same letter (weird throw away column expected by BERT)\n","    the text examples you want to classify\n","\n","test.tsv\n","\n","    an ID for the row\n","    the text sentences/paragraph you want to test\n","\n","The below python code snippet would read the HackerEarth training data (train.csv) and prepares it according to BERT model compliance."]},{"cell_type":"code","metadata":{"id":"N1X78c3F6Ldp","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from pandas import DataFrame\n"," \n","le = LabelEncoder()\n"," \n","df = pd.read_csv(\"/content/train.csv\")\n"," \n","# Creating train and dev dataframes according to BERT\n","df_bert = pd.DataFrame({'user_id':df['User_ID'],\n","            'label':le.fit_transform(df['Is_Response']),\n","            'alpha':['a']*df.shape[0],\n","            'text':df['Description'].replace(r'\\n',' ',regex=True)})\n"," \n","df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n"," \n","# Creating test dataframe according to BERT\n","df_test = pd.read_csv(\"/content/test.csv\")\n","df_bert_test = pd.DataFrame({'User_ID':df_test['User_ID'],\n","                 'text':df_test['Description'].replace(r'\\n',' ',regex=True)})\n"," \n","# Saving dataframes to .tsv format as required by BERT\n","df_bert_train.to_csv('/content/data/train.tsv', sep='\\t', index=False, header=False)\n","df_bert_dev.to_csv('/content/data/dev.tsv', sep='\\t', index=False, header=False)\n","df_bert_test.to_csv('/content/data/test.tsv', sep='\\t', index=False, header=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLW4OegQ7bqh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"32c394ff-cbb6-4754-d01f-d3da242a24fc","executionInfo":{"status":"ok","timestamp":1589271794436,"user_tz":-330,"elapsed":972,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["df_bert_test.head(3)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User_ID</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id80132</td>\n","      <td>Looking for a motel in close proximity to TV t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id80133</td>\n","      <td>Walking distance to Madison Square Garden and ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id80134</td>\n","      <td>Visited Seattle on business. Spent - nights in...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   User_ID                                               text\n","0  id80132  Looking for a motel in close proximity to TV t...\n","1  id80133  Walking distance to Madison Square Garden and ...\n","2  id80134  Visited Seattle on business. Spent - nights in..."]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"s4Fmyjlp8S6V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"db76ec44-2535-4401-b672-d8812c0d3273","executionInfo":{"status":"ok","timestamp":1589271798798,"user_tz":-330,"elapsed":1000,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["df_bert_train.head(3)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>label</th>\n","      <th>alpha</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>14154</th>\n","      <td>id24480</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>I would have to say this was nicer than what I...</td>\n","    </tr>\n","    <tr>\n","      <th>26936</th>\n","      <td>id37262</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>This Embassy Suites hotel is definitely nicer ...</td>\n","    </tr>\n","    <tr>\n","      <th>33400</th>\n","      <td>id43726</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>My husband and I spent five nights at this hot...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       user_id  label alpha                                               text\n","14154  id24480      0     a  I would have to say this was nicer than what I...\n","26936  id37262      0     a  This Embassy Suites hotel is definitely nicer ...\n","33400  id43726      0     a  My husband and I spent five nights at this hot..."]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"mwmjARQ-8Vre","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"a991c24b-e64e-4b73-cbe5-ec26cf47b3b1","executionInfo":{"status":"ok","timestamp":1589271801348,"user_tz":-330,"elapsed":1026,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["df_bert_dev.head(3)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>label</th>\n","      <th>alpha</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>36036</th>\n","      <td>id46362</td>\n","      <td>1</td>\n","      <td>a</td>\n","      <td>Terrible stay. Loud, tiny room. Incredibly ove...</td>\n","    </tr>\n","    <tr>\n","      <th>27418</th>\n","      <td>id37744</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>My now husband and I started staying at this S...</td>\n","    </tr>\n","    <tr>\n","      <th>2623</th>\n","      <td>id12949</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>The hotel looks avergae on the outside but the...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       user_id  label alpha                                               text\n","36036  id46362      1     a  Terrible stay. Loud, tiny room. Incredibly ove...\n","27418  id37744      0     a  My now husband and I started staying at this S...\n","2623   id12949      0     a  The hotel looks avergae on the outside but the..."]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"ncjlDVZa8_G4","colab_type":"text"},"source":["A column of all the same letter — this is a throw-away column that you need to include because the BERT model expects it. In our case it is the alpha column"]},{"cell_type":"markdown","metadata":{"id":"OehIqt6j9XWJ","colab_type":"text"},"source":["Training Model using Pre-trained BERT model\n","\n","Following the blog-post till here finishes half of the job. Just recheck the following things.\n","\n","    All the .tsv files are in a folder having name “data”\n","    Make sure you have created a folder “bert_output” where the fine tuned model will be saved and test results are generated under the name “test_results.tsv“\n","    Check that you downloaded the pre-trained BERT model in current directory “cased_L-12_H-768_A-12”\n","    Also, ensure that the paths in the command are relative path (starts with “./”)\n","\n","One can now fine tune the downloaded pre-trained model for our problem data-set by running the below command on terminal:"]},{"cell_type":"code","metadata":{"id":"cWCpwF808ZNr","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/bert')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jktjASXA_feY","colab_type":"code","colab":{}},"source":["! python run_classifier.py --task_name=cola --do_train=true --do_eval=true --do_predict=True --data_dir='/content/data' --vocab_file='/content/cased_L-12_H-768_A-12/vocab.txt' --bert_config_file='/content/cased_L-12_H-768_A-12/bert_config.json' --init_checkpoint='/content/cased_L-12_H-768_A-12/bert_model.ckpt' --max_seq_length=400 --train_batch_size=8 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir='/content/bert_output' --do_lower_case=False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfBUNjwmAVh3","colab_type":"code","colab":{}},"source":["! pip install tensorflow-gpu==1.15"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBfel75qdUkB","colab_type":"text"},"source":["Preparing Results for Submission\n","\n","The below python code converts the results from BERT model to .csv format in order to submit to HackerEarth Challenge."]},{"cell_type":"code","metadata":{"id":"LmduJJPadb0v","colab_type":"code","colab":{}},"source":["df_results = pd.read_csv(\"/content/bert_output/test_results.tsv\",sep=\"\\t\",header=None)\n","df_results_csv = pd.DataFrame({'User_ID':df_test['User_ID'],\n","                               'Is_Response':df_results.idxmax(axis=1)})\n"," \n","# Replacing index with string as required for submission\n","df_results_csv['Is_Response'].replace(0, 'happy',inplace=True)\n","df_results_csv['Is_Response'].replace(1, 'not_happy',inplace=True)\n"," \n","# writing into .csv\n","df_results_csv.to_csv('/content/data/result.csv',sep=\",\",index=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"62VsCmCeduUL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"d6928f97-27be-414a-97d0-4516da55d65b","executionInfo":{"status":"ok","timestamp":1589281020670,"user_tz":-330,"elapsed":73919,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["! zip -r bert_output.zip '/content/bert_output'"],"execution_count":21,"outputs":[{"output_type":"stream","text":["  adding: content/bert_output/ (stored 0%)\n","  adding: content/bert_output/eval_results.txt (deflated 18%)\n","  adding: content/bert_output/predict.tf_record (deflated 80%)\n","  adding: content/bert_output/model.ckpt-14453.data-00000-of-00001 (deflated 13%)\n","  adding: content/bert_output/graph.pbtxt (deflated 97%)\n","  adding: content/bert_output/train.tf_record (deflated 80%)\n","  adding: content/bert_output/model.ckpt-14453.meta (deflated 92%)\n","  adding: content/bert_output/eval.tf_record (deflated 80%)\n","  adding: content/bert_output/events.out.tfevents.1589272608.2df7c7822df7 (deflated 90%)\n","  adding: content/bert_output/eval/ (stored 0%)\n","  adding: content/bert_output/eval/events.out.tfevents.1589279143.2df7c7822df7 (deflated 92%)\n","  adding: content/bert_output/model.ckpt-14453.index (deflated 69%)\n","  adding: content/bert_output/test_results.tsv (deflated 63%)\n","  adding: content/bert_output/.ipynb_checkpoints/ (stored 0%)\n","  adding: content/bert_output/checkpoint (deflated 75%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YFS_NPpxg-ij","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":130},"outputId":"9bb1e9e3-1d58-4b58-d323-3f79c622c409","executionInfo":{"status":"ok","timestamp":1589280806271,"user_tz":-330,"elapsed":39091,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bSk4ie_3iDDD","colab_type":"code","colab":{}},"source":["! mv '/content/bert/bert_output.zip' '/content/drive/My Drive/Colab Notebooks'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yw5x5wiOieYQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"predict_customer_happiness.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}