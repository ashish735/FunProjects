{"cells":[{"cell_type":"code","metadata":{"id":"-BQm4975_3vv","colab_type":"code","outputId":"41ec13a6-a0e7-4c29-c958-016f8ad8b084","executionInfo":{"status":"ok","timestamp":1589349905157,"user_tz":-330,"elapsed":30714,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":74}},"source":["! pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-563c9958-241d-4d70-a443-e4cf538f8662\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-563c9958-241d-4d70-a443-e4cf538f8662\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving kaggle.json to kaggle.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sTtukPoZcnMN","colab_type":"text"},"source":["https://appliedmachinelearning.blog/2017/06/14/voice-gender-detection-using-gmms-a-python-primer/"]},{"cell_type":"markdown","metadata":{"id":"YfHpFjY5a3iB","colab_type":"text"},"source":["# Voice Gender Detection using GMMs : A Python Primer\n","\n","The below data-sets can be downloaded from here.\n","\n","  Training corpus : It has been developed from YouTube videos and consists of 5 minutes of speech for each gender, spoken by 5 distinct male and 5 female speakers (i.e, 1 minute/speaker).\n","  \n","  Test corpus: It has been extracted from “AudioSet“, a large scale manually annotated corpus recently released by Google this year (2017). The subset constructed from it contains 558 female only speech utterances and 546 male only speech utterances. All audio files are of 10 seconds duration and are sampled at 16000 Hz.\n"]},{"cell_type":"markdown","metadata":{"id":"afSFwFzabQbB","colab_type":"text"},"source":["We will give a brief primer about how to work with speech signals. From the speech signals in training data, a popular speech feature, Mel Frequency Cepstrum Coefficients (MFCCs), will be extracted; they are known to contain gender information (among other things). The 2 gender models are built by using yet another famous ML technique – Gaussian Mixture Models (GMMs). A GMM will take as input the MFCCs of the training samples and will try to learn their distribution, which will be representative of the gender. Now, when the gender of a new voice sample is to be detected, first the MFCCs of the sample will be extracted and then the trained GMM models will be used to calculate the scores of the features for both the models. Model with the maximum score is predicted as gender of the test speech. Having given an overview of the approach, presented following is the organisation of this blog post:"]},{"cell_type":"markdown","metadata":{"id":"3z8xT6Vsbsrn","colab_type":"text"},"source":["# 1. Working with speech frames\n","\n","A speech signal is just a sequence of numbers which denote the amplitude of the speech spoken by the speaker. We need to understand 3 core concepts while working with speech signals:\n","\n","  Framing – Since speech is a non-stationary signal, its frequency contents are continuously changing with time. In order to do any sort of analysis of the signal, such as knowing its frequency contents for short time intervals (known as Short Term Fourier Transform of the signal), we need to be able to view it as a stationary signal. To achieve this stationarity, the speech signal is divided into short frames of duration 20 to 30 milliseconds, as the shape of our vocal tract can be assumed to be unvarying for such small intervals of time. Frames shorter than this duration won’t have enough samples to give a good estimate of the frequency components, while in longer frames the signal may change too much within the frame that the condition of stationary no more holds."]},{"cell_type":"markdown","metadata":{"id":"MdUe1gAfch2l","colab_type":"text"},"source":["Windowing – Extracting raw frames from a speech signal can lead to discontinuities towards the endpoints due to non-integer number of periods in the extracted waveform, which will then lead to an erroneous frequency representation (known as spectral leakage in signal processing lingo). This is prevented by multiplying a window function with the speech frame. A window function’s amplitude gradually falls to zero towards its two end and thus this multiplication minimizes the amplitude of the above mentioned discontinuities."]},{"cell_type":"markdown","metadata":{"id":"QHsyHDurdqjg","colab_type":"text"},"source":["Overlapping frames – Due to windowing, we are actually losing the samples towards the beginning and the end of the frame; this too will lead to an incorrect frequency representation. To compensate for this loss, we take overlapping frames rather than disjoint frames, so that the samples lost from the end of the ith frame and the beginning of the (i+1)th frame are wholly included in the frame formed by the overlap between these 2 frames. The overlap between frames is generally taken to be of 10-15 ms."]},{"cell_type":"markdown","metadata":{"id":"M4QfiQ2ueCME","colab_type":"text"},"source":["# 2. Extracting MFCC features\n","\n","Having extracted the speech frames, we now proceed to derive MFCC features for each speech frame.  Speech is produced by humans by filtering applied by our vocal tract on the air expelled by our lungs. The properties of the source (lungs) are common for all speakers; it is the properties of the vocal tract, which is responsible for giving shape to the spectrum of signal and it varies across speakers. The shape of the vocal tract governs what sound is produced and the MFCCs best represent this shape.\n","\n","MFCCs are mel-frequency cepstral coefficients which are some transformed values of signal in cepstral domain. From theory of speech production, speech is assumed to be convolution of source (air expelled from lungs) and filter (our vocal tract). The purpose here is to characterise the filter and remove the source part. In order to achieve this,"]},{"cell_type":"markdown","metadata":{"id":"lxZIzAJDeFXE","colab_type":"text"},"source":["\n","  We first transform the time domain speech signal into spectral domain signal using Fourier transform where source and filter part are now in multiplication.\n","\n","  Take log of the transformed values so that source and filter are now additive in log spectral domain. Use of log to transform from multiplication to summation made it easy to separate source and filter using a linear filter.\n","\n","  Finally, we apply discrete cosine transform (found to be more successful than FFT or I-FFT) of the log spectral signal to get MFCCs. Initially the idea was to transform the log spectral signal to time domain using Inverse-FFT but ‘log’ being a non-linear operation created new frequencies called Quefrency or say it transformed the log spectral signal into a new domain called cepstral domain (ceps being reverse of spec).\n","  \n","  The reason for the term ‘mel’ in MFFC is mel scale which exactly specifies how to space our frequency regions. Humans are much better at discerning small changes in pitch at low frequencies than they are at high frequencies. Incorporating this scale makes our features match more closely what humans hear.\n","\n","The above explanation is just to give the readers an idea of how these features were motivated. If you really like to explore and understand more about MFCCs, you can read in this blog. You can find various implementation for extracting MFCCs on internet. We have employed python_speech_features (check here). All you have to do is pip install python_speech_features. One can also install scikits talkbox for the same. The following python code is a function to extract MFCC features from given audio."]},{"cell_type":"code","metadata":{"id":"Q3vw8D5FfAON","colab_type":"code","colab":{}},"source":["import python_speech_features as mfcc\n","def get_MFCC(sr,audio):\n","    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = False)\n","    features = preprocessing.scale(features)\n","    return features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukhn6DGofmta","colab_type":"text"},"source":["# 3. Training gender models\n","\n","In order to build a gender detection system from the above extracted features, we need to model both the genders. We employ GMMs for this task.\n","\n","A Gaussian mixture model is a probabilistic clustering model for representing the presence of sub-populations within an overall population.  The idea of training a GMM is to approximate the probability distribution of a class by a linear combination of ‘k’ Gaussian distributions/clusters, also called the components of the GMM. The likelihood of data points (feature vectors) for a model is given by following equation:"]},{"cell_type":"markdown","metadata":{"id":"idZHS2l7fpRC","colab_type":"text"},"source":["Python’s sklearn.mixture package is used by us to learn a GMM from the features matrix containing the MFCC features. The GMM object requires the number of components n_components to be fitted on the data, the number of iterations n_iter to be performed for estimating the parameters of these n components, the type of co-variance covariance_type to be assumed between the features and the number of times n_ init the K-means initialization is to be done. The initialization which gave the best results is kept. The fit() function then estimates the model parameters using the EM algorithm.\n","\n","The following Python code is used to train the gender models. The code is run once for each gender and source is given the path to the training files for the respective gender."]},{"cell_type":"code","metadata":{"id":"S1wNzuXcf2VR","colab_type":"code","colab":{}},"source":["from sklearn.mixture import GMM\n","gmm = GMM(n_components = 8, n_iter = 200, covariance_type='diag',n_init = 3)\n","gmm.fit(features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CoYYSFU0gf6d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"70eb575f-80ba-4121-c6d2-009edb7bb23e","executionInfo":{"status":"ok","timestamp":1589516640964,"user_tz":-330,"elapsed":3616,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import os\n","import pickle\n","import numpy as np\n","from scipy.io.wavfile import read\n","from sklearn.mixture import GaussianMixture as GMM\n","import python_speech_features as mfcc\n","from sklearn import preprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"," \n","def get_MFCC(sr,audio):\n","    features = mfcc.mfcc(audio,sr, 0.025, 0.01, 13,appendEnergy = False)\n","    features = preprocessing.scale(features)\n","    return features\n"," \n","#path to training data\n","source   = \"/content/pygender/train_data/youtube/male\"\n","#path to save trained model\n","dest     = \"/content/pygender\"\n","files    = [os.path.join(source,f) for f in os.listdir(source) if\n","             f.endswith('.wav')]\n","features = np.asarray(());\n"," \n","for f in files:\n","    sr,audio = read(f)\n","    vector   = get_MFCC(sr,audio)\n","    if features.size == 0:\n","        features = vector\n","    else:\n","        features = np.vstack((features, vector))\n"," \n","gmm = GMM(n_components = 8, covariance_type='diag',\n","        n_init = 3)\n","gmm.fit(features)\n","picklefile = f.split(\"/\")[-2].split(\".wav\")[0]+\".gmm\"\n"," \n","# model saved as male.gmm\n","filename = '/content/pygender/Male.gmm'\n","outfile = open(filename,'wb')\n","pickle.dump(gmm, outfile)\n","outfile.close()\n","print('modeling completed for gender:',picklefile)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["modeling completed for gender: male.gmm\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X6VQR7kQj4bY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"9283029f-7c64-4a88-f6a6-bfa03f034c6a","executionInfo":{"status":"ok","timestamp":1589516658771,"user_tz":-330,"elapsed":3723,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import os\n","import pickle\n","import numpy as np\n","from scipy.io.wavfile import read\n","from sklearn.mixture import GaussianMixture as GMM\n","import python_speech_features as mfcc\n","from sklearn import preprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"," \n","def get_MFCC(sr,audio):\n","    features = mfcc.mfcc(audio,sr, 0.025, 0.01, 13,appendEnergy = False)\n","    features = preprocessing.scale(features)\n","    return features\n"," \n","#path to training data\n","source   = \"/content/pygender/train_data/youtube/female\"\n","#path to save trained model\n","dest     = \"/content/pygender\"\n","files    = [os.path.join(source,f) for f in os.listdir(source) if\n","             f.endswith('.wav')]\n","features = np.asarray(());\n"," \n","for f in files:\n","    sr,audio = read(f)\n","    vector   = get_MFCC(sr,audio)\n","    if features.size == 0:\n","        features = vector\n","    else:\n","        features = np.vstack((features, vector))\n"," \n","gmm = GMM(n_components = 8, covariance_type='diag',\n","        n_init = 3)\n","gmm.fit(features)\n","picklefile = f.split(\"/\")[-2].split(\".wav\")[0]+\".gmm\"\n"," \n","# model saved as male.gmm\n","filename = '/content/pygender/Female.gmm'\n","outfile = open(filename,'wb')\n","pickle.dump(gmm, outfile)\n","outfile.close()\n","print('modeling completed for gender:',picklefile)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["modeling completed for gender: female.gmm\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AwqcjldYkf1D","colab_type":"text"},"source":["# 4. Evaluation on subset of AudioSet corpus\n","\n","Upon arrival of a test voice sample for gender detection, we begin by extracting the MFCC features for it, with 25 ms frame size and 10 ms overlap between frames . Next we require the log likelihood scores for each frame of the sample, x_1, x_2, ... ,x_i , belonging to each gender, ie, P(x_i|female) and P(x_i|male) is to be calculated. Using (2), the likelihood of the frame being from a female voice is calculated by substituting the \\mu and \\Sigma of female GMM model. This is done for each of the k Gaussian components in the model, and the weighted  sum of the k likelihoods from the components is taken as per the w parameter of the model, just like in (1). The logarithm operation when applied on the obtained sum gives us the log likelihood value for the frame. This is repeated for all the frames of the sample and the likelihoods of all the frames are added.\n","\n","Similar to this, the likelihood of the speech being male is calculated by substituting the values of the parameters of the trained male GMM model and repeating the above procedure for all the frames. The Python code given below predicts the gender of the test audio."]},{"cell_type":"code","metadata":{"id":"WtqC2KU6hfcK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"9bbaa2af-ba46-414c-f58b-4f9c2c7b84eb","executionInfo":{"status":"ok","timestamp":1589517921588,"user_tz":-330,"elapsed":33133,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import os\n","import pickle\n","import numpy as np\n","from scipy.io.wavfile import read\n","import python_speech_features as mfcc\n","from sklearn import preprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","def get_MFCC(sr,audio):\n","    features = mfcc.mfcc(audio,sr, 0.025, 0.01, 13,appendEnergy = False)\n","    feat     = np.asarray(())\n","    for i in range(features.shape[0]):\n","        temp = features[i,:]\n","        if np.isnan(np.min(temp)):\n","            continue\n","        else:\n","            if feat.size == 0:\n","                feat = temp\n","            else:\n","                feat = np.vstack((feat, temp))\n","    features = feat;\n","    features = preprocessing.scale(features)\n","    return features\n"," \n","#path to test data\n","sourcepath = \"/content/pygender/test_data/AudioSet/female_clips\"\n","#path to saved models\n","modelpath  = \"/content/pygender\"    \n"," \n","gmm_files = [os.path.join(modelpath,fname) for fname in\n","              os.listdir(modelpath) if fname.endswith('.gmm')]\n","models    = [pickle.load(open(fname,'rb')) for fname in gmm_files]\n","genders   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname\n","              in gmm_files]\n","files     = [os.path.join(sourcepath,f) for f in os.listdir(sourcepath)\n","              if f.endswith(\".wav\")] \n","total_file=0\n","correct=0\n","wrong=0\n","for f in files:\n","    #print(f.split(\"/\")[-1])\n","    total_file+=1\n","    sr, audio  = read(f)\n","    features   = get_MFCC(sr,audio)\n","    scores     = None\n","    log_likelihood = np.zeros(len(models))\n","    for i in range(len(models)):\n","        gmm    = models[i]         #checking with each model one by one\n","        scores = np.array(gmm.score(features))\n","        log_likelihood[i] = scores.sum()\n","    winner = np.argmax(log_likelihood)\n","    if(genders[winner]=='Female'):\n","      correct+=1\n","    else:\n","      wrong+=1\n","    #print(\"\\tdetected as - \", genders[winner],\"\\n\\tscores:female \",log_likelihood[0],\",male \", log_likelihood[1],\"\\n\")\n","print(\"total no of files = \",total_file)\n","print(\"Correctly identified files = \", correct)\n","print(\"Wrong identification = \", wrong)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["total no of files =  558\n","Correctly identified files =  531\n","Wrong identification =  27\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZK96uskjiyP0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"f19e48b1-568b-4d15-bcca-96a53bb6c678","executionInfo":{"status":"ok","timestamp":1589518156703,"user_tz":-330,"elapsed":32558,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import os\n","import pickle\n","import numpy as np\n","from scipy.io.wavfile import read\n","import python_speech_features as mfcc\n","from sklearn import preprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","def get_MFCC(sr,audio):\n","    features = mfcc.mfcc(audio,sr, 0.025, 0.01, 13,appendEnergy = False)\n","    feat     = np.asarray(())\n","    for i in range(features.shape[0]):\n","        temp = features[i,:]\n","        if np.isnan(np.min(temp)):\n","            continue\n","        else:\n","            if feat.size == 0:\n","                feat = temp\n","            else:\n","                feat = np.vstack((feat, temp))\n","    features = feat;\n","    features = preprocessing.scale(features)\n","    return features\n"," \n","#path to test data\n","sourcepath = \"/content/pygender/test_data/AudioSet/male_clips\"\n","#path to saved models\n","modelpath  = \"/content/pygender\"    \n"," \n","gmm_files = [os.path.join(modelpath,fname) for fname in\n","              os.listdir(modelpath) if fname.endswith('.gmm')]\n","models    = [pickle.load(open(fname,'rb')) for fname in gmm_files]\n","genders   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname\n","              in gmm_files]\n","files     = [os.path.join(sourcepath,f) for f in os.listdir(sourcepath)\n","              if f.endswith(\".wav\")] \n","total_file=0\n","correct=0\n","wrong=0\n","for f in files:\n","    #print(f.split(\"/\")[-1])\n","    total_file+=1\n","    sr, audio  = read(f)\n","    features   = get_MFCC(sr,audio)\n","    scores     = None\n","    log_likelihood = np.zeros(len(models))\n","    for i in range(len(models)):\n","        gmm    = models[i]         #checking with each model one by one\n","        scores = np.array(gmm.score(features))\n","        log_likelihood[i] = scores.sum()\n","    winner = np.argmax(log_likelihood)\n","    if(genders[winner]=='Male'):\n","      correct+=1\n","    else:\n","      wrong+=1\n","    #print(\"\\tdetected as - \", genders[winner],\"\\n\\tscores:female \",log_likelihood[0],\",male \", log_likelihood[1],\"\\n\")\n","print(\"total no of files = \",total_file)\n","print(\"Correctly identified files = \", correct)\n","print(\"Wrong identification = \", wrong)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["total no of files =  546\n","Correctly identified files =  414\n","Wrong identification =  132\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kv3DRrJCrI5U","colab_type":"text"},"source":["http://www.primaryobjects.com/2016/06/22/identifying-the-gender-of-a-voice-using-machine-learning/"]},{"cell_type":"markdown","metadata":{"id":"tSqtO7bGrLSW","colab_type":"text"},"source":["Concluding Remarks\n","\n","We hope the blog post was successful in explaining to you your first speech processing task. We expect you to reproduce the results posted by us. In order to get a better understanding of the various parameters used by us, you can play with them, for example:\n","\n","  Train gender models on larger data-set. In the experiments performed, training data consists of  5 minutes of speech per gender only. A larger data-set may improve the accuracy as it will encompass the MFCCs well.\n","\n","  Clean the extracted data-set from AudioSet. We have not done any cleaning or noise removal. Few of the audios are noisy and the spoken speech part in audio is less in the 10 seconds audio clip.\n","\n","  Readers can study the effect of number of GMM components on the performance of the models.\n","\n","  We have taken only MFCCs in order to build a gender detection system. In literature, you can find lot of acoustic features that are used to build gender model. For reference here is a link.\n"]},{"cell_type":"code","metadata":{"id":"bCp9LUZqrRrO","colab_type":"code","colab":{}},"source":["a=10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xB9YPhrnvfD4","colab_type":"text"},"source":["# Spoken Speaker Identification based on Gaussian Mixture Models : Python Implementation"]},{"cell_type":"markdown","metadata":{"id":"aH4bS5tIvxyq","colab_type":"text"},"source":["Similar to our previous post “Voice Gender Detection“, this blog-post focuses on a beginner’s method to answer the question ‘who is the speaker‘ in the speech file. Recently, lot of voice biometric systems have been developed which can extract speaker information from the recorded voice and identify the speaker from set of trained speakers in the database. In this blog-post, we will illustrate the same with a naive approach using Gaussian Mixture Models (GMM). There are other conventional as well as modern approaches which are more robust to channel noise and also performs better than approach followed in this blog-post.\n","\n","    GMM-UBM (Gaussian Mixture Model – Universal Background Model) using MAP (Maximum Aposteriori) adaptation [1] is one of the successful conventional technique to implement speaker identification.\n","    I-vectors based speaker identification [2] is the state-of-the-art technique implemented in lot of voice biometric products.\n","\n","As a beginner, the above mentioned techniques may overwhelm you as they are mathematically complex methods and requires some research effort in order to comprehend. Therefore, I am not following any of the two approaches. Instead, I am interested in showing you the implementation of fundamental step of speaker identification (using GMMs) which can then lead to development of GMM-UBM or I-vectors approach."]},{"cell_type":"markdown","metadata":{"id":"Zrh7I1enwX6b","colab_type":"text"},"source":["The below data-sets can be downloaded from here.\n","\n","  Training corpus : It has been developed from audios taken from ‘on-line VoxForge speech database’ and consists of 5 speech utterances for each speaker, spoken by 34 speakers (i.e, 20-30 seconds/speaker).\n","\n","  Test corpus: This consists of remaining 5 unseen utterances of the same 34 speakers taken in train corpus.  All audio files are of 10 seconds duration and are sampled at 16000 Hz.\n"]},{"cell_type":"markdown","metadata":{"id":"1iyMJg3Mw5oD","colab_type":"text"},"source":["I will strongly recommend you to read our previous post ‘Voice Gender Detection’ as a brief primer about how to work with speech signals are explained there. We have also discussed about extracting a popular speech feature, Mel Frequency Cepstrum Coefficients (MFCCs) previously. A GMM will take as input the MFCCs and derivatives of MFCCs of the training samples of a speaker and will try to learn their distribution, which will be representative of that speaker. A typical speaker identification process can be shown by flow diagram below."]},{"cell_type":"markdown","metadata":{"id":"UPs95dlTw8yW","colab_type":"text"},"source":["While testing when the speaker of a new voice sample is to be identified, first the 40-dimensional feature (MFCCs + delta MFCC) of the sample will be extracted and then the trained speaker GMM models will be used to calculate the scores of the features for all the models. Speaker model with the maximum score is predicted as the identified speaker of the test speech. Having said that we will go through the python implementation of the following steps:\n","\n","    40-Dimensional Feature Extraction\n","    Training Speaker Models.\n","    Evaluating Performance on test set\n"]},{"cell_type":"markdown","metadata":{"id":"m51Kzj1ZxFR1","colab_type":"text"},"source":["Feature Extraction.\n","\n","We extract 40-dimensional features from speech frames. There are 20 MFCC features and 20 derivatives of MFCC features. The derivatives of MFCCs provides the information of dynamics of MFCCs over the time. It turns out that calculating the delta-MFCC and appending them to the original MFCC features (20-dimenaionl) increases the performance in lot of speech analytics applications. To calculate delta features from MFCCs, we apply the following equation."]},{"cell_type":"markdown","metadata":{"id":"fyag1y61xKWg","colab_type":"text"},"source":["The below python functions extracts MFCC features and derives delta coefficients from from audio signal."]},{"cell_type":"code","metadata":{"id":"qFuwdph2wON1","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn import preprocessing\n","import python_speech_features as mfcc\n"," \n","def calculate_delta(array):\n","    \"\"\"Calculate and returns the delta of given feature vector matrix\"\"\"\n","    rows,cols = array.shape\n","    deltas = np.zeros((rows,20))\n","    N = 2\n","    for i in range(rows):\n","        index = []\n","        j = 1\n","        while(j <= N):\n","            if(i-j < 0):\n","                first = 0\n","            else:\n","                first = i-j\n","            if(i+j > rows -1):\n","                second = rows -1\n","            else:\n","                second = i+j\n","            index.append((second,first))\n","            j+=1\n","        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n","    return deltas\n"," \n","def extract_features(audio,rate):\n","    \"\"\"extract 20 dim mfcc features from an audio, performs CMS and combines\n","    delta to make it 40 dim feature vector\"\"\"   \n"," \n","    mfcc_feat = mfcc.mfcc(audio,rate, 0.025, 0.01,20,appendEnergy = True)\n","    mfcc_feat = preprocessing.scale(mfcc_feat)\n","    delta = calculate_delta(mfcc_feat)\n","    combined = np.hstack((mfcc_feat,delta))\n","    return combined"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_bAjZ2adxOaq","colab_type":"text"},"source":["2. Training Speaker Models.\n","\n","As we know, There are 34 distinct speakers in training corpus which are taken from lots of speaker provided by VoxForge. The path of all the audio files (5 per speaker) utilized for training are given in this file. Usually there is a very important step called pre-processing, aslo known as voice activity detection(VAD) which includes noise removal and silence truncation from the audios. I have assumed that there is no requirement of performing VAD here.\n","\n","In order to build a speaker identification system from the above extracted features, we need to model all the speakers independently now. We employ GMMs for this task.\n","\n","A Gaussian mixture model is a probabilistic clustering model for representing the presence of sub-populations within an overall population.  The idea of training a GMM is to approximate the probability distribution of a class by a linear combination of ‘k’ Gaussian distributions/clusters, also called the components of the GMM. The likelihood of data points (feature vectors) for a model is given by following equation:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zi6OH_t3x1gi","colab_type":"text"},"source":["Python’s sklearn.mixture package is used by us to learn a GMM from the features matrix containing the 40 dimensional MFCC and delta-MFCC features. More about sklearn GMM can be read from section 3 of our previous post ‘Voice Gender Detection‘. The following Python code is used to train the GMM speaker models with 16 components. The code is run once for each speaker and train_file is variable which has text filename containing path to all the audios for the respective speaker. Also, you have to create a “speaker_models” directory where all the models will be dumped after training."]},{"cell_type":"code","metadata":{"id":"bBSvsd3xxia4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":649},"outputId":"dd05173d-a96d-4113-8437-1778f2d440f8","executionInfo":{"status":"ok","timestamp":1589521149860,"user_tz":-330,"elapsed":40489,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import pickle\n","import numpy as np\n","from scipy.io.wavfile import read\n","from sklearn.mixture import GaussianMixture as GMM\n","import warnings\n","import numpy as np\n","from sklearn import preprocessing\n","import python_speech_features as mfcc\n","warnings.filterwarnings(\"ignore\")\n"," \n","#path to training data\n","source   = \"/content/development_set/\"  \n"," \n","#path where training speakers will be saved\n","dest = \"/content/speaker_models/\"\n","train_file = \"/content/development_set_enroll.txt\"\n","file_paths = open(train_file,'r')\n"," \n","count = 1\n","# Extracting features for each speaker (5 files per speakers)\n","features = np.asarray(())\n","for path in file_paths:\n","    path = path.strip()\n","    #print(path)\n","    path=path.replace(\"\\\\\", \"/\")\n","    # read the audio\n","    sr,audio = read(source + path)\n"," \n","    # extract 40 dimensional MFCC & delta MFCC features\n","    vector   = extract_features(audio,sr)\n"," \n","    if features.size == 0:\n","        features = vector\n","    else:\n","        features = np.vstack((features, vector))\n","    # when features of 5 files of speaker are concatenated, then do model training\n","    if count == 5:\n","        gmm = GMM(n_components = 16, covariance_type='diag',n_init = 3)\n","        gmm.fit(features)\n"," \n","        # dumping the trained gaussian model\n","        picklefile = path.split(\"-\")[0]+\".gmm\"\n","        pickle.dump(gmm,open(dest + picklefile,'wb'))\n","        print('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)\n","        features = np.asarray(())\n","        count = 0\n","    count = count + 1"],"execution_count":43,"outputs":[{"output_type":"stream","text":["+ modeling completed for speaker: anthonyschaller.gmm  with data point =  (1595, 40)\n","+ modeling completed for speaker: Apple_Eater.gmm  with data point =  (2077, 40)\n","+ modeling completed for speaker: Ara.gmm  with data point =  (3644, 40)\n","+ modeling completed for speaker: argail.gmm  with data point =  (2468, 40)\n","+ modeling completed for speaker: ariyan.gmm  with data point =  (2506, 40)\n","+ modeling completed for speaker: arjuan.gmm  with data point =  (2707, 40)\n","+ modeling completed for speaker: Artem.gmm  with data point =  (2794, 40)\n","+ modeling completed for speaker: arthur.gmm  with data point =  (3782, 40)\n","+ modeling completed for speaker: artk.gmm  with data point =  (3369, 40)\n","+ modeling completed for speaker: arun.gmm  with data point =  (3031, 40)\n","+ modeling completed for speaker: arvala.gmm  with data point =  (2718, 40)\n","+ modeling completed for speaker: asalkeld.gmm  with data point =  (3294, 40)\n","+ modeling completed for speaker: asladic.gmm  with data point =  (1743, 40)\n","+ modeling completed for speaker: AslakKnutsen.gmm  with data point =  (2444, 40)\n","+ modeling completed for speaker: asp.gmm  with data point =  (2506, 40)\n","+ modeling completed for speaker: AT.gmm  with data point =  (2694, 40)\n","+ modeling completed for speaker: atamur.gmm  with data point =  (2056, 40)\n","+ modeling completed for speaker: ataru80.gmm  with data point =  (2831, 40)\n","+ modeling completed for speaker: atterer.gmm  with data point =  (2391, 40)\n","+ modeling completed for speaker: audioodyssey.gmm  with data point =  (3219, 40)\n","+ modeling completed for speaker: avsa242.gmm  with data point =  (2708, 40)\n","+ modeling completed for speaker: ax.gmm  with data point =  (2719, 40)\n","+ modeling completed for speaker: axllaruse.gmm  with data point =  (3381, 40)\n","+ modeling completed for speaker: azmisov.gmm  with data point =  (2195, 40)\n","+ modeling completed for speaker: B.gmm  with data point =  (2042, 40)\n","+ modeling completed for speaker: bachroxx.gmm  with data point =  (1955, 40)\n","+ modeling completed for speaker: bae.gmm  with data point =  (2719, 40)\n","+ modeling completed for speaker: Bahoke.gmm  with data point =  (2194, 40)\n","+ modeling completed for speaker: Bareford.gmm  with data point =  (2844, 40)\n","+ modeling completed for speaker: bart.gmm  with data point =  (2079, 40)\n","+ modeling completed for speaker: Bassel.gmm  with data point =  (2831, 40)\n","+ modeling completed for speaker: beady.gmm  with data point =  (2409, 40)\n","+ modeling completed for speaker: beez1717.gmm  with data point =  (2132, 40)\n","+ modeling completed for speaker: belmontguy.gmm  with data point =  (2594, 40)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7iGh0Mws2eRY","colab_type":"text"},"source":["## 3. Evaluating Performance on Test set.\n","\n","Test set consists of 5 unseen utterances of trained 34 speakers. The path of all the audio files (5 per speaker) utilized for evaluation are given in this file.\n","\n","Upon arrival of a test voice sample for speaker identification, we begin by extracting the 40 dimensional for it, with 25 ms frame size and 10 ms overlap between frames. Next we require the log likelihood scores for each frame of the sample, x_1, x_2, ... ,x_i , belonging to each speaker, ie, P(x_i|S_j) (for all j that belongs to S) is to be calculated. The likelihood of the frame being from a particular speaker is calculated by substituting the \\mu and \\Sigma of that speaker GMM model in likelihood equation shown in previous section. This is done for each of the ‘k’ Gaussian components in the model, and the weighted  sum of the ‘k’ likelihoods from the components is taken as per the weight ‘w ‘ parameter of the model. The logarithm operation when applied on the obtained sum gives us the log likelihood value for the frame. This is repeated for all the frames of the sample and the likelihoods of all the frames are added. The speaker model with highest likelihood score is considered as the identified speaker.\n","\n","The Python code given below predicts the speaker of the test audio."]},{"cell_type":"code","metadata":{"id":"mughqwKCzOR6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"68f9af68-3e0c-48be-87ee-e2dd1a0b002e","executionInfo":{"status":"ok","timestamp":1589522296342,"user_tz":-330,"elapsed":11115,"user":{"displayName":"Ashish Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giv7IjFwPubM89y3gRmwHAL5JyL7B_sSLxDip_t=s64","userId":"15381583435702060119"}}},"source":["import os\n","import pickle\n","import numpy as np\n","from scipy.io.wavfile import read\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import time\n"," \n","#path to training data\n","source   = \"/content/development_set/\"\n","modelpath = \"/content/speaker_models/\"\n","test_file = \"/content/development_set_test.txt\"\n","file_paths = open(test_file,'r')\n"," \n","gmm_files = [os.path.join(modelpath,fname) for fname in\n","              os.listdir(modelpath) if fname.endswith('.gmm')]\n"," \n","#Load the Gaussian gender Models\n","models    = [pickle.load(open(fname,'rb')) for fname in gmm_files]\n","speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname\n","              in gmm_files]\n","total_file=0\n","correct=0\n","wrong=0\n","# Read the test directory and get the list of test audio files\n","for path in file_paths:   \n"," \n","    path = path.strip()\n","    #print(path)\n","    path=path.replace(\"\\\\\", \"/\")\n","    name = path.split('-')[0]\n","    total_file+=1\n","    sr,audio = read(source + path)\n","    vector   = extract_features(audio,sr)\n"," \n","    log_likelihood = np.zeros(len(models)) \n"," \n","    for i in range(len(models)):\n","        gmm    = models[i]  #checking with each model one by one\n","        scores = np.array(gmm.score(vector))\n","        log_likelihood[i] = scores.sum()\n"," \n","    winner = np.argmax(log_likelihood)\n","    if(name==speakers[winner]):\n","      correct+=1\n","    else:\n","      wrong+=1\n","    #print(\"\\tdetected as - \", speakers[winner])\n","    #time.sleep(1.0)\n","print(\"total no of files = \",total_file)\n","print(\"Correctly identified files = \", correct)\n","print(\"Wrong identification = \", wrong)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["total no of files =  170\n","Correctly identified files =  169\n","Wrong identification =  1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hs1wLuhU3mtY","colab_type":"text"},"source":["'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n","\n","if the above error occurs in pickle.load, it is because it was dumped in r format and was loaded in rb format. That is format mismatch. Load and dump in same format to remove the error"]},{"cell_type":"markdown","metadata":{"id":"4miH1tvf4jWY","colab_type":"text"},"source":["Results and Conclusion\n","\n","This beginner’s approach performs with an in-set accuracy of 100%, identifying all the 170 speech utterances correctly. There are few reasons for such perfect result.\n","\n","  The unseen utterances of speakers taken from VoxForge are possibly of same channel or environment.\n","\n","  The evaluation task in performed on small dataset. Consider a data inflow where you are getting probably some thousands of calls in a day.\n","\n","  Consider the situation when we have to identify speakers from the set of 1000 speakers.\n","\n","  In this evaluation, we have not taken out-of-set speakers into account i.e. if the audio is not from any speaker still our system will identify it as one of speakers in trained set depending upon highest likelihood.\n","\n","  In the real environment, we may get more noisy and unclean data. Speaker identification system needs to be robust.\n"]},{"cell_type":"code","metadata":{"id":"IiUO9JOy3_8G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py"},"colab":{"name":"Basic_layout.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":0}